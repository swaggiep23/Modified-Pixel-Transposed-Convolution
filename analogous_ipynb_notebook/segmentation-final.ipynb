{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZCM65CBt1CJ"
      },
      "source": [
        "## Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\")\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCSP-dbMw88x"
      },
      "source": [
        "# Image segmentation\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/segmentation\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMP7mglMuGT2"
      },
      "source": [
        "This python notebook focuses on the task of image segmentation, using a modified U-Net and the Pixel Transposed Convolution (PixelTCL) as introduced by Gao et al in [this paper](https://arxiv.org/abs/1705.06820).\n",
        "\n",
        "---\n",
        "## What is image segmentation?\n",
        "\n",
        "In an image classification task, the network assigns a label (or class) to each input image. However, suppose you want to know the shape of that object, which pixel belongs to which object, etc. In this case, you need to assign a class to each pixel of the imageâ€”this task is known as segmentation. A segmentation model returns much more detailed information about the image. Image segmentation has many applications in medical imaging, self-driving cars and satellite imaging, just to name a few.\n",
        "\n",
        "This tutorial primarily uses the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) ([Parkhi et al, 2012](https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)). The dataset consists of images of 37 pet breeds, with 200 images per breed (~100 each in the training and test splits). Each image includes the corresponding labels, and pixel-wise masks. The masks are class-labels for each pixel. Each pixel is given one of three categories:\n",
        "\n",
        "* Class 1: Pixel belonging to the pet.\n",
        "* Class 2: Pixel bordering the pet.\n",
        "* Class 3: None of the above/a surrounding pixel.\n",
        "\n",
        "\n",
        "The network can also be trained on the [Pascal VOC2012 dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).\n",
        "\n",
        "This dataset has 21 classes (including the background), and in addition to that there is a border class marked in the original version of the dataset. The border pixels are removed in [this version](https://www.kaggle.com/datasets/swagatpanda23/voc2012mod/versions/6), and therefore it contains exactly 21 classes. Some points to note when training a model with this dataset.\n",
        "* This is a hard dataset to train compared to the Oxford-IIIT pets dataset because:\n",
        "    * the number of classes is larger\n",
        "    * multiple classes often appear in the same image, at a varying frequency\n",
        "    * 75% of the pixels are background pixels, i.e., the dataset is unbalanced.\n",
        "* It requires more training time compared to the Oxford-IIIT pets dataset due to a much larger number of images (~3000 in the former to ~10,500 in the latter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HVoSpfr3jOZ"
      },
      "source": [
        "---\n",
        "## Installations and Imports\n",
        "\n",
        "As usual, we would import all the relevant python modules that we would use to implement, train and test our neural network.\n",
        "\n",
        "We first import os and make a variable to keep note of our working environment, which will allow us to use if-else cases such that the same code can be run in tensorflow as well as the local device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibrEtGwxsvs0",
        "outputId": "039ec39e-0d98-453d-b37c-8dc7b56c4e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'bede4a9b56a9\\n'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    inColab = True\n",
        "    inKaggle = not inColab\n",
        "elif os.getenv(\"KAGGLE_URL_BASE\"):\n",
        "    inKaggle = True\n",
        "    inColab = not inKaggle\n",
        "else: # if you're using a local machine\n",
        "    inColab = False\n",
        "    inKaggle = False\n",
        "\n",
        "host_name_str = os.popen('hostname').read().encode('utf-8')\n",
        "print(host_name_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUR7YOaHPKfP"
      },
      "source": [
        "### Installations depending on where the code is running\n",
        "P.S. Comment out the segmentation models, tensorflow-hub and tensorflow-addons imports and uses since they weren't particularly helpful in our pursuits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pxbGgMxpPYAg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if inKaggle:\n",
        "    !pip install tensorflow==2.9.0\n",
        "#     !pip install --upgrade tensorflow-addons\n",
        "#     !pip install -U segmentation-models\n",
        "    !pip install focal-loss\n",
        "    !add-apt-repository -y ppa:deadsnakes/ppa\n",
        "    !apt install -y python3\n",
        "    !pip install protobuf==3.19.*\n",
        "    !yes | apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "\n",
        "    !pip install --upgrade pip\n",
        "    !pip install --upgrade \"jax[cuda]\"\n",
        "    # !pip install --upgrade tensorflow_hub\n",
        "\n",
        "elif inColab:\n",
        "    %env SM_FRAMEWORK=tf.keras\n",
        "#     !pip install tensorflow-addons\n",
        "#     !pip install segmentation-models\n",
        "    !pip install focal-loss\n",
        "    # !pip install --upgrade tensorflow_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSRhYQbY32k9"
      },
      "source": [
        "### Importing the necessary libraries\n",
        "Keras Modules are imported from tensorflow furthermore to reduce the length of the code while using the relevant predefined functions and therefore improve the readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9mQeD3YPEtqI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "# import tensorflow_hub as hub\n",
        "# import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import cv2\n",
        "import functools\n",
        "import gc\n",
        "# import segmentation_models as sm\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import models\n",
        "from keras import backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVzFo3vz54nq"
      },
      "source": [
        "---\n",
        "## Loading and Splitting the Dataset\n",
        "The dataset is perhaps the most important pre-requisite to a neural network, so we pre process it such that it is easier to use with the neural network that we are going to use, adding functions of relevance for visualization during training and testing, as well as splitting it appropriately between training and testing cases.\n",
        "\n",
        "The dataset is [available from TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). The segmentation masks are included in version 3+.\n",
        "\n",
        "We have the option of using the tfds dataset, which is fairly straightforward if the dataset that you're working with is available on it. In addition, we can also load the dataset using TensorFlow functions exclusively, as implemented below.\n",
        "\n",
        "The dataset already contains the required training and test splits, so continue to use the same splits, which is implemented in the latter part of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FD60EbcAQqov"
      },
      "outputs": [],
      "source": [
        "def load_and_split_dataset(BATCH_SIZE, BUFFER_SIZE, load_from_tfds, img_dir,\n",
        "                           mask_dir, train_txt_path, val_txt_path, input_shape):\n",
        "    \"\"\"\n",
        "    BATCH_SIZE: (Integer)\n",
        "        As obvious, the batch size is to be specified.\n",
        "\n",
        "    BUFFER_SIZE: (Integer)\n",
        "        As obvious, the buffer size is to be specified.\n",
        "\n",
        "    load_from_tfds: [Boolean]\n",
        "        A variable to check if the dataset is\n",
        "        loaded using tfds or from a custom pipeline.\n",
        "\n",
        "    img_dir: (String)\n",
        "        The path to the directory that contains training and validation images.\n",
        "\n",
        "    mask_dir: (String)\n",
        "        The path to the directory that contains the training and\n",
        "        validation labelled masks.\n",
        "\n",
        "    train_path_txt: (String)\n",
        "        The path to the directory that contains the list of images\n",
        "        belonging to the training datasets.\n",
        "\n",
        "    val_path_txt: (String)\n",
        "        The path to the directory that contains the list of images\n",
        "        belonging to the validation datasets.\n",
        "\n",
        "    input_shape: (Tuple)\n",
        "        The 2-dimensional shape of the input images that will make the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    if load_from_tfds:\n",
        "        dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
        "        train_images = dataset['train']\\\n",
        "                       .map(functools.partial(load_image,\n",
        "                                              input_shape=input_shape),\n",
        "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        validation_images = dataset['test']\\\n",
        "                            .map(functools.partial(load_image,\n",
        "                                                   input_shape=input_shape),\n",
        "                                 num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "        VAL_LENGTH = info.splits['test'].num_examples\n",
        "    else:\n",
        "        train_data =[]\n",
        "        train_mask =[]\n",
        "        with open(train_txt_path) as f:\n",
        "            for line in f:\n",
        "                train_data.append(os.path.join(img_dir,\n",
        "                                               line.split()[0]+'.jpg'))\n",
        "                train_mask.append(os.path.join(mask_dir,\n",
        "                                               line.split()[0]+'.png'))\n",
        "        validation_data =[]\n",
        "        validation_mask =[]\n",
        "        with open(val_txt_path) as f:\n",
        "            for line in f:\n",
        "                validation_data.append(os.path.join(img_dir,\n",
        "                                                    line.split()[0]+'.jpg'))\n",
        "                validation_mask.append(os.path.join(mask_dir,\n",
        "                                                    line.split()[0]+'.png'))\n",
        "        train_images = tf.data.Dataset.from_tensor_slices((train_data,\n",
        "                                                           train_mask))\n",
        "        validation_images = tf.data.Dataset.from_tensor_slices((validation_data,\n",
        "                                                                validation_mask))\n",
        "        train_images = train_images.map(functools.partial(load_image_alter,\n",
        "                                                          input_shape=input_shape),\n",
        "                                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        validation_images = validation_images.map(functools.partial(load_image_alter,\n",
        "                                                                    input_shape=input_shape),\n",
        "                                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        TRAIN_LENGTH = len(train_data)\n",
        "        VAL_LENGTH = len(validation_data)\n",
        "    train_batches = (train_images\n",
        "                    # .cache()\n",
        "                    .shuffle(BUFFER_SIZE)\n",
        "                    .batch(BATCH_SIZE)\n",
        "                    .repeat()\n",
        "                    .map(Augment())\n",
        "                    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "    validation_batches = validation_images.batch(BATCH_SIZE)\n",
        "    return train_batches, validation_batches, TRAIN_LENGTH, VAL_LENGTH\n",
        "\n",
        "\n",
        "def load_and_split_dataset_test(BATCH_SIZE, BUFFER_SIZE, load_from_tfds, img_dir,\n",
        "                                mask_dir, train_txt_path, val_txt_path, input_shape,\n",
        "                                predData_path):\n",
        "    \"\"\"\n",
        "    BATCH_SIZE: (Integer)\n",
        "        As obvious, the batch size is to be specified.\n",
        "\n",
        "    BUFFER_SIZE: (Integer)\n",
        "        As obvious, the buffer size is to be specified.\n",
        "\n",
        "    load_from_tfds: [Boolean]\n",
        "        A variable to check if the dataset is\n",
        "        loaded using tfds or from a custom pipeline.\n",
        "\n",
        "    img_dir: (String)\n",
        "        The path to the directory that contains training and validation images.\n",
        "\n",
        "    mask_dir: (String)\n",
        "        The path to the directory that contains the training and\n",
        "        validation labelled masks.\n",
        "\n",
        "    train_path_txt: (String)\n",
        "        The path to the directory that contains the list of images\n",
        "        belonging to the training datasets.\n",
        "\n",
        "    val_path_txt: (String)\n",
        "        The path to the directory that contains the list of images\n",
        "        belonging to the validation datasets.\n",
        "\n",
        "    input_shape: (Tuple)\n",
        "        The 2-dimensional shape of the input images that will make the dataset.\n",
        "\n",
        "    predData_path: (String)\n",
        "        The path to the directory that contains images that are meant to be tested.\n",
        "    \"\"\"\n",
        "\n",
        "    if load_from_tfds:\n",
        "        dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
        "        train_images = dataset['train']\\\n",
        "                       .map(functools.partial(load_image,\n",
        "                                              input_shape=input_shape),\n",
        "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        validation_images = dataset['test']\\\n",
        "                            .map(functools.partial(load_image,\n",
        "                                                   input_shape=input_shape),\n",
        "                                 num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "        VAL_LENGTH = info.splits['test'].num_examples\n",
        "\n",
        "    else:\n",
        "        train_data =[]\n",
        "        train_mask =[]\n",
        "        with open(train_txt_path) as f:\n",
        "            for line in f:\n",
        "                train_data.append(os.path.join(img_dir,\n",
        "                                               line.split()[0]+'.jpg'))\n",
        "                train_mask.append(os.path.join(mask_dir,\n",
        "                                               line.split()[0]+'.png'))\n",
        "        validation_data =[]\n",
        "        validation_mask =[]\n",
        "        with open(val_txt_path) as f:\n",
        "            for line in f:\n",
        "                validation_data.append(os.path.join(img_dir,\n",
        "                                                    line.split()[0]+'.jpg'))\n",
        "                validation_mask.append(os.path.join(mask_dir,\n",
        "                                                    line.split()[0]+'.png'))\n",
        "        train_images = tf.data.Dataset.from_tensor_slices((train_data,\n",
        "                                                           train_mask))\n",
        "        validation_images = tf.data.Dataset.from_tensor_slices((validation_data,\n",
        "                                                                validation_mask))\n",
        "        train_images = train_images.map(functools.partial(load_image_alter,\n",
        "                                                          input_shape=input_shape),\n",
        "                                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        validation_images = validation_images.map(functools.partial(load_image_alter,\n",
        "                                                                    input_shape=input_shape),\n",
        "                                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        TRAIN_LENGTH = len(train_data)\n",
        "        VAL_LENGTH = len(validation_data)\n",
        "\n",
        "    train_batches = (train_images\n",
        "                    # .cache()\n",
        "                    .shuffle(BUFFER_SIZE)\n",
        "                    .batch(BATCH_SIZE)\n",
        "                    .repeat()\n",
        "                    .map(Augment())\n",
        "                    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "    validation_batches = validation_images.batch(BATCH_SIZE)\n",
        "\n",
        "    pred_data = os.listdir(predData_path)\n",
        "    pred_data_complete = []\n",
        "    for elem in pred_data:\n",
        "        pred_data_complete.append(os.path.join(predData_path, elem))\n",
        "\n",
        "    pred_images = tf.data.Dataset.from_tensor_slices((pred_data_complete))\n",
        "    pred_images = pred_images.map(functools.partial(load_image_test,\n",
        "                                                    input_shape=input_shape),\n",
        "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    PRED_LENGTH = len(pred_data)\n",
        "    pred_batches = pred_images.batch(1)\n",
        "    return train_batches, validation_batches, pred_batches, TRAIN_LENGTH, VAL_LENGTH, PRED_LENGTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFduewJdCXj"
      },
      "source": [
        "---\n",
        "## Image Processing Utility Functions\n",
        "These are the functions that assist in loading the images into the dataset so that they can be processed as tensors all the same (i.e. all of them have same dimensions and normalized pixel values in our case), as well as adding to the dataset by augmenting them.\n",
        "\n",
        "\n",
        "* `normalize()` function normalizes the image by converting all of the pixel values between 0 and 1 for the input image, and offsets all of the labelled mask pixel values by 1, so that instead of them being [1, 2, 3] for our case, they are [0, 1, 2].\n",
        "*  `load_image()` function - Reads the input image and normalizes it.\n",
        "* `load_image_alter()` function - Same as `load_image()`, except it is used when loading the dataset from a directory, directly (i.e. without using tfds).\n",
        "* The class `Augment()` performs a simple augmentation by randomly-flipping an image horizontally. (subclass to the `tf.keras.layer.Layer`)\n",
        "The image and the mask use the same seed so that whenever random flipping occurs, it is for both of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1bj7SPKIRw_I"
      },
      "outputs": [],
      "source": [
        "def normalize(input_image, input_mask):\n",
        "    input_image = tf.cast(input_image, tf.float32) / 127.5 - 1\n",
        "    input_mask -= 1\n",
        "    input_mask = tf.cast(input_mask, tf.float32)\n",
        "    return input_image, input_mask\n",
        "\n",
        "\n",
        "def normalize_test(input_image):\n",
        "    input_image = tf.cast(input_image, tf.float32) / 127.5 - 1\n",
        "    return input_image\n",
        "\n",
        "\n",
        "def load_image(datapoint, input_shape):\n",
        "    input_image = tf.image.resize(datapoint['image'], input_shape)\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'],\n",
        "                                 input_shape,\n",
        "                                 # tuple(int(elem/2) for elem in input_shape),\n",
        "                                 method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "    return input_image, input_mask\n",
        "\n",
        "\n",
        "def load_image_test(image_path, input_shape):\n",
        "    \"\"\"\n",
        "    Used when the dataset is loaded using tfds.\n",
        "    \"\"\"\n",
        "    data_image = tf.io.read_file(image_path)\n",
        "    data_image = tf.io.decode_jpeg(data_image, channels=3)\n",
        "    data_image = tf.image.resize(data_image, input_shape)\n",
        "    data_image = normalize_test(data_image)\n",
        "    return data_image\n",
        "\n",
        "\n",
        "# This function is used only when the dataset isn't loaded using tfds.\n",
        "def load_image_alter(image_path, mask_path, input_shape):\n",
        "    data_image = tf.io.read_file(image_path)\n",
        "    data_image = tf.io.decode_jpeg(data_image, channels=3)\n",
        "    data_image = tf.image.resize(data_image, input_shape)\n",
        "\n",
        "    mask_image = tf.io.read_file(mask_path)\n",
        "    mask_image = tf.io.decode_png(mask_image, channels=1)\n",
        "    mask_image = tf.image.resize(mask_image, input_shape, # tuple(int(elem/2) for elem in input_shape),\n",
        "                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    data_image, mask_image = normalize(data_image, mask_image)\n",
        "    return data_image, mask_image\n",
        "\n",
        "\n",
        "class Augment(keras.layers.Layer):\n",
        "    def __init__(self, seed=20):\n",
        "        super().__init__()\n",
        "        # both use the same seed, so they'll make the same random changes.\n",
        "        self.augment = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
        "\n",
        "    def call(self, inputs, labels):\n",
        "        # The label data is converted to uint8 so that we can concatenate it with the inputs with datatype float32.\n",
        "        # labels = tf.image.convert_image_dtype(labels, dtype=tf.float32)\n",
        "        output = self.augment(layers.concatenate([inputs, labels], axis=-1))\n",
        "        # This is because the data that I am using has 3 channels for inputs and 1 channel for labels.\n",
        "        inputs = output[:,:,:,0:3]\n",
        "        labels = output[:,:,:,3:]\n",
        "        # Labels converted back to uint8\n",
        "        # labels = tf.image.convert_image_dtype(labels, dtype=tf.uint8)\n",
        "        return inputs, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "---\n",
        "## Define the model\n",
        "The model being used here is a modified [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler).\n",
        "\n",
        "*To learn robust features and reduce the number of trainable parameters, the Tensorflow documentation uses a pretrained modelâ€”[MobileNetV2](https://arxiv.org/abs/1801.04381)â€”as the encoder. For the decoder, it uses an upsample block, which is already implemented in the [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) example in the TensorFlow Examples repo. (Check out the [pix2pix: Image-to-image translation with a conditional GAN](../generative/pix2pix.ipynb) tutorial in a notebook.)*\n",
        "\n",
        "For this particular case though, **we would be using a standard U-net that is implemented in `DenseUNet`, and a more sophisticated U-net that is implemented in `pixelUNet`**, in a comparative manner. The former is as described, a standard U-net structure that uses `pixelTCL()` and `iPixelTCL()` in a way that is discussed in [Pixel Deconvolutional Networks](https://arxiv.org/pdf/1705.06820.pdf).\n",
        "\n",
        "The following classes simplify the implementation of\n",
        "* `BatchActivate()`: Batch Normalization followed by Relu Activation.\n",
        "* `adsConv2D()`: Atrous Depthwise Convolution - A two step convolution, that reduces the number of trainable parameters.\n",
        "    * `Depthwise`: With a given kernel size, and the number of filters remains the same.\n",
        "    * `Pointwise`: With a (1, 1) kernel, and the number of filters are the number of filters desired by the output.\n",
        "* `atrousSPP()`: Atrous Spatial Pyramidal Pooling - Convolutions with different kernel sizes provided as inputs which are finally concatenated.\n",
        "* `DenseLayer()`: Dense Layer - A sequence of batchnorm -> relu activation -> convolution -> dropout, adapted from [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326v3.pdf), which has been subsequently modified after adaptation from [Densely connected convolutional networks](https://arxiv.org/pdf/1608.06993.pdf).\n",
        "* `dilate_tensor()`: Dilate tensor - Adds `upsample_factor`-1 number of zero rows and columns to a feature map. The positioning of the non-zero row/column depends on the `row_shift`/`column_shift` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "60dNsQVHC10U"
      },
      "outputs": [],
      "source": [
        "class BatchActivate(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Batchnormalization of the input followed by a ReLU activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, activation='relu'):\n",
        "        super().__init__()\n",
        "        self.activation =  activation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if inKaggle:\n",
        "            self.batchnorm = layers.BatchNormalization()\n",
        "        elif inColab:\n",
        "            self.batchnorm = layers.BatchNormalization(synchronized=True)\n",
        "        self.activate = layers.Activation(self.activation)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activate(self.batchnorm(inputs))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                        \"activation\": self.activation\n",
        "                     })\n",
        "        return config\n",
        "\n",
        "\n",
        "class adsConv2D(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Atrous Depthwise Separable 2D Convolution: https://arxiv.org/pdf/1802.02611.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, kernel_depthwise, padding='valid',\n",
        "                 strides=1, activation=None, dilation_rate=1,\n",
        "                 kernel_initializer='he_uniform'):\n",
        "        super().__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_depthwise = kernel_depthwise\n",
        "        self.padding = padding\n",
        "        self.strides = strides\n",
        "        self.activation = activation\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.pointwise = layers.Conv2D(filters=self.filters,\n",
        "                                       kernel_size=(1, 1),\n",
        "                                       activation=self.activation,\n",
        "                                       kernel_initializer=self.kernel_initializer)\n",
        "        self.depthwise = layers.DepthwiseConv2D(kernel_size=self.kernel_depthwise,\n",
        "                                                padding=self.padding,\n",
        "                                                activation=self.activation,\n",
        "                                                strides=self.strides,\n",
        "                                                dilation_rate=self.dilation_rate,\n",
        "                                                kernel_initializer=self.kernel_initializer)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.pointwise(self.depthwise(inputs))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                        \"filters\": self.filters,\n",
        "                        \"kernel_depthwise\": self.kernel_depthwise,\n",
        "                        \"padding\": self.padding,\n",
        "                        \"strides\": self.strides,\n",
        "                        \"activation\": self.activation,\n",
        "                        \"dilation_rate\": self.dilation_rate,\n",
        "                        \"kernel_initializer\": self.kernel_initializer\n",
        "                     })\n",
        "        return config\n",
        "\n",
        "\n",
        "class atrousSPP(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Atrous Spatial Pyramidal Pooling, also implemented in Deeplabv3+:\n",
        "    https://arxiv.org/pdf/1802.02611.pdf\n",
        "\n",
        "    kernel_size: (Tuple of Integers) The kernel size used for the parallel convolutions\n",
        "                  before concatenation.\n",
        "    in_filters: (Integer) The number of filters in the input (due to this particular implementation).\n",
        "    channel_axis: (Integer) The channel axis for all of the input tensors.\n",
        "    num_outputs: (Integer) The number of channels desired in the output of this layer.\n",
        "    conv_option: (String) To choose the convolution type, either \"conv2d\" or \"adsconv2d\".\n",
        "    pyramid_dilations: (List of Integers) The dilation values for the parallel convolutions.\n",
        "                      (for both the dimensions of a 2D image)\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size, in_filters, channel_axis,\n",
        "                 num_outputs, conv_option, pyramid_layers):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.in_filters = in_filters\n",
        "        self.channel_axis = channel_axis\n",
        "        self.num_outputs = num_outputs\n",
        "        self.conv_option = conv_option\n",
        "        self.pyramid_layers = pyramid_layers\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv_list = []\n",
        "        for dilation in self.pyramid_layers:\n",
        "            self.conv_list.append(adsConv2D(self.in_filters, self.kernel_size,\n",
        "                                            padding='same', dilation_rate=dilation))\n",
        "        self.convf = conv2_op(self.conv_option)(self.num_outputs, (1, 1),\n",
        "                                                padding='same')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.convf(layers.concatenate([conv(inputs) for conv in self.conv_list],\n",
        "                          axis=self.channel_axis))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                        \"kernel_size\": self.kernel_size,\n",
        "                        \"in_filters\": self.in_filters,\n",
        "                        \"channel_axis\": self.channel_axis,\n",
        "                        \"num_outputs\": self.num_outputs,\n",
        "                        \"conv_option\": self.conv_option,\n",
        "                        \"pyramid_layers\": self.pyramid_layers\n",
        "                     })\n",
        "        return config\n",
        "\n",
        "\n",
        "class DenseLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    This class implements the Dense layer comprised of the following:\n",
        "    Batchnormalization -> Relu activation -> Convolution (depthwise or built-in) -> Dropout.\n",
        "\n",
        "    This layer is discussed in great detail in the following paper:\n",
        "    https://arxiv.org/pdf/1611.09326.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, growth_rate, kernel_dense, dropout,\n",
        "                 conv_option):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "        kernel_dense: (Tuple) the kernel size used for convolutions and deconvolutions.\n",
        "        growth_rate: (Integer) The number of feature maps for each layer of the denseblock.\n",
        "        dropout: (Integer) The dropout value for the denseblock.\n",
        "        conv_option: (String) Sets the convolution type. Either 'conv2d' or 'adsconv2d'.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        self.kernel_dense = kernel_dense\n",
        "        self.dropout = dropout\n",
        "        self.conv_option = conv_option\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.dropout = layers.Dropout(self.dropout)\n",
        "        self.conv = conv2_op(self.conv_option)(self.growth_rate,\n",
        "                                               self.kernel_dense,\n",
        "                                               padding='same')\n",
        "        self.batchact = BatchActivate(activation=tf.keras.layers.PReLU())\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: Input tensors.\n",
        "        \"\"\"\n",
        "        return self.dropout(self.conv(self.batchact(inputs)))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                    \"growth_rate\": self.growth_rate,\n",
        "                    \"kernel_dense\": self.kernel_dense,\n",
        "                    \"dropout\": self.dropout,\n",
        "                    \"conv_option\": self.conv_option\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class dilate_tensor(keras.layers.Layer):\n",
        "    def __init__(self, axis, row_shift, column_shift,\n",
        "                 upsample_factor):\n",
        "        super().__init__()\n",
        "        self.axis = axis\n",
        "        self.row_shift = row_shift\n",
        "        self.column_shift = column_shift\n",
        "        self.upsample_factor = upsample_factor\n",
        "\n",
        "    def call(self, inputs):\n",
        "        row_shifts = [item for item in range(self.upsample_factor)]\n",
        "        row_shifts.remove(self.row_shift)\n",
        "        rows = tf.unstack(inputs, axis=self.axis[0])\n",
        "        row_zeros = tf.zeros_like(rows[0], dtype=tf.float32)\n",
        "\n",
        "        for step, rshift in enumerate(row_shifts):\n",
        "            for index in range(len(rows), 0, -(step+1)):\n",
        "                rows.insert(index-rshift, row_zeros)\n",
        "        inputs = tf.stack(rows, axis=self.axis[0])\n",
        "\n",
        "        column_shifts = [item for item in range(self.upsample_factor)]\n",
        "        column_shifts.remove(self.column_shift)\n",
        "        columns = tf.unstack(inputs, axis=self.axis[1])\n",
        "        columns_zeros = tf.zeros_like(columns[0], dtype=tf.float32)\n",
        "\n",
        "        for step, cshift in enumerate(column_shifts):\n",
        "            for index in range(len(columns), 0, -(step+1)):\n",
        "                columns.insert(index-cshift, columns_zeros)\n",
        "        inputs = tf.stack(columns, axis=self.axis[1])\n",
        "        return inputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                        \"axis\": self.axis,\n",
        "                        \"row_shift\": self.row_shift,\n",
        "                        \"column_shift\": self.column_shift,\n",
        "                        \"upsample_factor\": self.upsample_factor,\n",
        "                     })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8bNvDi8FCVI"
      },
      "source": [
        "The following cell implements simpler functions for convolutions and downsampling that will be used in the UNet.\n",
        "* `batchnorm()`: Batchnormalization for the entire code, had to add it due to the changes in the syntax of *'synchronized=True'* in Tensorflow 2.12.\n",
        "* `conv2_op()`: General Convolution - Chooses from `adsConv2D` and `Conv2D` and provides with a conv object as an output.\n",
        "* `downsample2_op()`: General Pooling - Chooses from `MaxPooling` and `conv2_op` with stride 2 and provides with a pool object as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KzBScMZlFCVI"
      },
      "outputs": [],
      "source": [
        "def BatchNorm(inputs):\n",
        "    \"\"\"\n",
        "    Configures the Batchnormalization for the entire code. In case you have Tensorflow 2.12 or above installed on your machine,\n",
        "    change the if else conditions to meet your requirements.\n",
        "    \"\"\"\n",
        "    if inColab:\n",
        "        outputs = layers.BatchNormalization(synchronized=True)(inputs)\n",
        "    else:\n",
        "        outputs = layers.BatchNormalization()(inputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def conv2_op(conv_option):\n",
        "    if conv_option == 'conv2d':\n",
        "        def conv_in(filters, kernel_size,\n",
        "                    strides=1, padding='valid',\n",
        "                    dilation_rate=1, activation=None,\n",
        "                    kernel_initializer='he_uniform'):\n",
        "            return tf.keras.layers.Conv2D(filters, kernel_size,\n",
        "                                          strides=strides,\n",
        "                                          padding=padding,\n",
        "                                          dilation_rate=dilation_rate,\n",
        "                                          activation=activation,\n",
        "                                          kernel_initializer=kernel_initializer)\n",
        "    elif conv_option == 'sepconv2d':\n",
        "        # The built-in layer that I wasn't aware of.\n",
        "        def conv_in(filters, kernel_size,\n",
        "                    strides=1, padding='valid',\n",
        "                    dilation_rate=1, activation=None,\n",
        "                    kernel_initializer='he_uniform'):\n",
        "            return tf.keras.layers.SeparableConv2D(filters, kernel_size,\n",
        "                                                   strides=strides,\n",
        "                                                   padding=padding,\n",
        "                                                   dilation_rate=dilation_rate,\n",
        "                                                   activation=activation,\n",
        "                                                   kernel_initializer=kernel_initializer)\n",
        "    elif conv_option == 'adsconv2d':\n",
        "        # The layer that I made, because I didn't know there was a built-in layer to do the same thing.\n",
        "        def conv_in(filters, kernel_size,\n",
        "                    strides=1, padding='valid',\n",
        "                    dilation_rate=1, activation=None,\n",
        "                    kernel_initializer='he_uniform'):\n",
        "            return adsConv2D(filters, kernel_size,\n",
        "                             strides=strides,\n",
        "                             padding=padding,\n",
        "                             dilation_rate=dilation_rate,\n",
        "                             activation=activation,\n",
        "                             kernel_initializer=kernel_initializer)\n",
        "\n",
        "#     elif conv_option == 'ipixel' or 'pixel':\n",
        "#         def conv_in(out_num, kernel_size, upsample_rate=2, strides=1,\n",
        "#                     padding='same', dilation_rate=1, d_format='NHWC'):\n",
        "#             return customCL(out_num, kernel_size, upsample_rate, strides,\n",
        "#                             padding, dilation_rate, d_format)\n",
        "    return conv_in\n",
        "\n",
        "\n",
        "def downsample2_op(conv_option, pool_option):\n",
        "    if pool_option == 'pool':\n",
        "        def downsample_in(num_outputs):\n",
        "            return layers.MaxPooling2D((2, 2))\n",
        "    elif pool_option == 'conv':\n",
        "        def downsample_in(num_outputs):\n",
        "            return conv2_op(conv_option)(num_outputs, (2, 2),\n",
        "                                         padding='same', strides=2)\n",
        "    return downsample_in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRsjdZuEnZfA"
      },
      "source": [
        "### PixelTCL and iPixelTCL\n",
        "The following is the implementation of a UNet that either uses regular deconvolution, pixelTCL or ipixelTCL as discussed in [Pixel Deconvolutional Networks](https://arxiv.org/pdf/1705.06820.pdf). These implementations do not require the structure of the UNet to be changed, and therefore can be instantiated mutually exclusively using an if else statement within the up convolution part of the UNet.\n",
        "\n",
        "The dilate tensor function is to be used in the `pixel_tcl` and `ipixel_tcl`  functions for the final step of doubling the size of the input tensor using the implementation trick discussed in the mentioned paper, which reduces the training and testing time.\n",
        "\n",
        "The down, bottom and the up blocks are the building blocks of the UNet that is implemented in the `pixelUNet()`.\n",
        "\n",
        "\n",
        "*Note that the number of filters on the last layer is set to the number of `output_channels`. This will be one output channel per class.*\n",
        "\n",
        "<hr style=\"border-style: dotted;\" />\n",
        "\n",
        "Before that however, we need to define our custom transposed convolution operations (which are often mistakenly called deconvolutions, which is a different operation that is supposed to be the mathematical inverse of convolution, which is why the name TCL is used for all of the code in this notebook). The following block of code defines these two pixelTCLs.\n",
        "\n",
        "`pixelTCL` is the implementation of what was discussed in the source material (the paper linked in the description above).\n",
        "\n",
        "`pixelTCL2` is a modification of the pixelTCL scheme, where each of the convolutions is independent of each other and directly linked to the input. These intermediate outputs are shuffled and combined to produce a larger feature map, similar to what the original paper did, except for the fact that all of these convolutions can be run parallely since they are independent of each other, whereas in the original paper all of the convolutions had to be sequential.\n",
        "The following diagram explains what goes on in pixelTCL2:\n",
        "\n",
        "![Feature map final.png](attachment:9b7f1ccd-9530-4358-b40a-87e72c749dc3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cA89AlUkX7pA"
      },
      "outputs": [],
      "source": [
        "class customTCL:\n",
        "    \"\"\"\n",
        "    Upsamples the input by 'upsample_rate' using the methods discussed in:\n",
        "    https://arxiv.org/pdf/1705.06820.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, tcl_type, out_num, kernel_size,\n",
        "                 upsample_rate, conv_option, d_format='NHWC'):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "\n",
        "        inputs: (4D tensor)\n",
        "            Input tensor, with the dimensions.\n",
        "            (batch_size, input_height, input_width, out_num)\n",
        "\n",
        "        out_num: (integer)\n",
        "            Output channel number.\n",
        "\n",
        "        kernel_size: (2-tuple of integers)\n",
        "            Convolutional kernel size.\n",
        "        \"\"\"\n",
        "        self.tcl_type = tcl_type\n",
        "        self.out_num = out_num\n",
        "        self.kernel_size = kernel_size\n",
        "        self.upsample_rate = upsample_rate\n",
        "        self.conv_option = conv_option\n",
        "        self.d_format = d_format\n",
        "        self.image_dim = 2\n",
        "\n",
        "    def TCL(self, inputs):\n",
        "        axis =  (self.d_format.index('H'), self.d_format.index('W'))\n",
        "        channel_axis = self.d_format.index('C')\n",
        "        loop_inputs = inputs\n",
        "        dilated_outputs = []\n",
        "        for index in range(self.upsample_rate**self.image_dim):\n",
        "            column_index = index%self.upsample_rate\n",
        "            row_index = int(index/self.upsample_rate)%self.upsample_rate\n",
        "            loop_inputs = BatchActivate(activation='relu')(loop_inputs)\n",
        "            conv = conv2_op(self.conv_option)(self.out_num,\n",
        "                                         self.kernel_size,\n",
        "                                         padding='same')(loop_inputs)\n",
        "            dilated_outputs.append(dilate_tensor(axis,\n",
        "                                                 row_index,\n",
        "                                                 column_index,\n",
        "                                                 self.upsample_rate)(conv))\n",
        "            loop_inputs = conv if index==0 and self.tcl_type == 'pixel' \\\n",
        "                          else layers.concatenate([loop_inputs, conv],\n",
        "                                                  axis=channel_axis)\n",
        "        outputs = tf.add_n(dilated_outputs)\n",
        "        return outputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "        ***************\n",
        "        *** Returns ***\n",
        "        ***************\n",
        "\n",
        "        outputs: (4D tensor)\n",
        "            Output tensor, with the dimensions.\n",
        "            (batch_size, upsample_rate*input_height, upsample_rate*input_width, out_num)\n",
        "        \"\"\"\n",
        "        return self.TCL(inputs)\n",
        "\n",
        "class customTCL2:\n",
        "    \"\"\"\n",
        "    Upsamples the input by 'upsample_rate', similar to `customTCL`,\n",
        "    except in this case all of the convolutions are independent of each other, unlike customTCL where\n",
        "    all of the operations are sequential.\n",
        "    \"\"\"\n",
        "    def __init__(self, tcl_type, out_num, kernel_size,\n",
        "                 upsample_rate, conv_option, d_format='NHWC'):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "\n",
        "        inputs: (4D tensor)\n",
        "            Input tensor, with the dimensions.\n",
        "            (batch_size, input_height, input_width, out_num)\n",
        "\n",
        "        out_num: (integer)\n",
        "            Output channel number.\n",
        "\n",
        "        kernel_size: (2-tuple of integers)\n",
        "            Convolutional kernel size.\n",
        "        \"\"\"\n",
        "        self.tcl_type = tcl_type\n",
        "        self.out_num = out_num\n",
        "        self.kernel_size = kernel_size\n",
        "        self.upsample_rate = upsample_rate\n",
        "        self.conv_option = conv_option\n",
        "        self.d_format = d_format\n",
        "        self.image_dim = 2\n",
        "\n",
        "    def TCL(self, inputs):\n",
        "        axis =  (self.d_format.index('H'), self.d_format.index('W'))\n",
        "        channel_axis = self.d_format.index('C')\n",
        "        loop_inputs = inputs\n",
        "        dilated_outputs = []\n",
        "        for index in range(self.upsample_rate**self.image_dim):\n",
        "            column_index = index%self.upsample_rate\n",
        "            row_index = int(index/self.upsample_rate)%self.upsample_rate\n",
        "            loop_inputs = BatchActivate(activation='relu')(loop_inputs)\n",
        "            conv = conv2_op(self.conv_option)(self.out_num,\n",
        "                                         self.kernel_size,\n",
        "                                         padding='same')(loop_inputs)\n",
        "            dilated_outputs.append(dilate_tensor(axis,\n",
        "                                                 row_index,\n",
        "                                                 column_index,\n",
        "                                                 self.upsample_rate)(conv))\n",
        "        outputs = tf.add_n(dilated_outputs)\n",
        "        return outputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "        ***************\n",
        "        *** Returns ***\n",
        "        ***************\n",
        "\n",
        "        outputs: (4D tensor)\n",
        "            Output tensor, with the dimensions.\n",
        "            (batch_size, upsample_rate*input_height, upsample_rate*input_width, out_num)\n",
        "        \"\"\"\n",
        "        return self.TCL(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDsLU33cFCVJ"
      },
      "source": [
        "**Modified U-Net models**\n",
        "\n",
        "The code for this project is a sophisticated U-Net with the following attributes:\n",
        "* An atrous spatial pyramidal pooling (ASPP) bottleneck layer.\n",
        "* Down-sampling blocks composed of dense blocks (consecutive dense layers with concatenation as described in [this paper by Jegou et al](https://arxiv.org/pdf/1611.09326.pdf)) followed by a down-sampling layer (which are convolution based instead of max-pooling based to improve the performance).\n",
        "* Up-sampling blocks are composed of up-sampling layer (either of the following: TCL, PixelTCL, iPixelTCL or modified iPixelTCL) followed by a dense layer.\n",
        "* The overall configuration is exactly the same as a standard/regular U-Net, the modifications occur within the up, down and bottleneck blocks.\n",
        "* The output layer is a softmax function.\n",
        "\n",
        "\n",
        "**Control models**\n",
        "\n",
        "We use two control models for this system\n",
        "1. The Standard U-Net, from Ronneberger et al.\n",
        "2. Deeplab Xception, the model that has performed the best in segmentation of Pascal VOC2012 dataset.\n",
        "\n",
        "These models, along with the Dense UNet, is contained within the same class, named the `modelClass`. These models are to compare the results with the dense UNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ck59iuOHFCVJ"
      },
      "outputs": [],
      "source": [
        "class modelClass:\n",
        "    def __init__(self, input_shape, network_depth, tcl_type,\n",
        "                 out_classnum, filtersize, dense_layers, growth_rate,\n",
        "                 dropout, conv_option, pool_option, pyramid_layers,\n",
        "                 d_format='NHWC'):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "\n",
        "        input_shape: (Tuple of integers)\n",
        "            Three dimensions representing the input shape in the form:\n",
        "            (input_height, input_width, input_channels)\n",
        "\n",
        "        network_depth: (Integer)\n",
        "            Depth of the Unet.\n",
        "\n",
        "        tcl_type: (String)\n",
        "            'builtin' for normal deconvolution, 'pixel' for pixel deconvolution,\n",
        "            'ipixel' ipixel deconvolution.\n",
        "\n",
        "        out_classnum: (Integer)\n",
        "            The number of output classes.\n",
        "\n",
        "        filtersize: (Integer)\n",
        "            The number of filters/channels for the first layer of the Unet.\n",
        "\n",
        "        dense_layers: (List of integers)\n",
        "           The number of dense layers in the denseblock in the Unet\n",
        "           at the depth corresponding to the list index.\n",
        "\n",
        "        growth_rate: (Integer)\n",
        "            The number of kernels/filters in each layer of\n",
        "            the denseblock.\n",
        "\n",
        "        dropout: (Integer)\n",
        "            The dropout value for the denseblock.\n",
        "\n",
        "        conv_option: (String)\n",
        "            Sets the convolution type. Either 'conv2d' or 'adsconv2d'.\n",
        "\n",
        "        pool_option: (String)\n",
        "            Sets the pooling option. Either 'builtin' or 'conv'.\n",
        "\n",
        "        pyramid_layers: (List of integers)\n",
        "            The dilation values for atrous convolution that will be\n",
        "            used in atrousSPP.\n",
        "\n",
        "        d_format: (String)\n",
        "            The data format of the input tensors - NHWC or NHCW.\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.network_depth = network_depth\n",
        "        self.tcl_type = tcl_type\n",
        "        self.out_classnum = out_classnum\n",
        "        self.filtersize = filtersize\n",
        "        self.dense_layers = dense_layers\n",
        "        self.growth_rate = growth_rate\n",
        "        self.dropout = dropout\n",
        "        self.conv_option = conv_option\n",
        "        self.pool_option = pool_option\n",
        "        self.pyramid_layers = pyramid_layers\n",
        "        self.d_format = d_format\n",
        "\n",
        "    def DenseBlock(self, inputs, dense_layers, growth_rate,\n",
        "                   kernel_dense, dropout, channel_axis, conv_option):\n",
        "        layer_input = inputs\n",
        "        dense = []\n",
        "        for dense_layer in range(dense_layers):\n",
        "            denseouts = DenseLayer(growth_rate, kernel_dense,\n",
        "                                   dropout, conv_option)(layer_input)\n",
        "            dense.append(denseouts)\n",
        "            layer_input = layers.concatenate([layer_input, denseouts],\n",
        "                                             axis=channel_axis)\n",
        "        dense_final = layers.concatenate([dense_item for dense_item in dense],\n",
        "                                         axis=channel_axis)\n",
        "        dense_concat = layers.concatenate([inputs, dense_final],\n",
        "                                          axis=channel_axis)\n",
        "        return dense_final, dense_concat\n",
        "\n",
        "    def ResnetBlock(self, inputs, repeats, filters,\n",
        "                    kernel_dense, dropout, channel_axis, conv_option):\n",
        "        layer_input = inputs\n",
        "        for dense_layer in range(repeats):\n",
        "            denseout1 = DenseLayer(filters, kernel_dense,\n",
        "                                   dropout, conv_option)(layer_input)\n",
        "            denseout2 = DenseLayer(filters, kernel_dense,\n",
        "                                   dropout, conv_option)(denseout1)\n",
        "            layer_input = layers.Add()([layer_input, denseout2])\n",
        "        return layer_input\n",
        "\n",
        "    def TransitionDown(self, inputs, num_outputs, dropout,\n",
        "                       conv_option, pool_option):\n",
        "        conv = DenseLayer(num_outputs, (1, 1), dropout, conv_option)(inputs)\n",
        "        conv = downsample2_op(conv_option, pool_option)(num_outputs)(conv)\n",
        "        return conv\n",
        "\n",
        "    def TransitionUp(self, inputs, tcl_type, num_outputs, conv_option,\n",
        "                     kernel_size_u, upsample_rate):\n",
        "        if tcl_type == 'builtin':\n",
        "            convt = layers.Conv2DTranspose(num_outputs, upsample_rate,\n",
        "                                           strides=upsample_rate, padding='same',\n",
        "                                           kernel_initializer='he_uniform')(inputs)\n",
        "#             convt = layers.UpSampling2D(size=upsample_rate, data_format=\"channels_last\",\n",
        "#                                         interpolation=\"bilinear\")(inputs)\n",
        "        elif tcl_type == 'ipixel' or tcl_type == 'pixel':\n",
        "            convt = customTCL(tcl_type, num_outputs, kernel_size_u, upsample_rate,\n",
        "                              conv_option, d_format='NHWC')(inputs)\n",
        "        elif tcl_type == 'modified ipixel':\n",
        "            convt = customTCL2(tcl_type, num_outputs, kernel_size_u, upsample_rate,\n",
        "                  conv_option, d_format='NHWC')(inputs)\n",
        "        return convt\n",
        "\n",
        "    def down_block_dense(self, inputs, down_outputs, kernel_size_d,\n",
        "                         channel_axis, dense_layers, layer_index, growth_rate,\n",
        "                         dropout, conv_option, pool_option,  pyramid_layers,\n",
        "                         isFinal=False):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "\n",
        "        down_outputs: (List of tensors)\n",
        "            The list of tensors that contains all the downsampled inputs, which are to be\n",
        "            concatenated to the processed input tensor in the up block.\n",
        "\n",
        "        kernel_size_d: (Tuple)\n",
        "            The kernel size used for convolutions and deconvolutions.\n",
        "\n",
        "        ***************\n",
        "        *** Returns ***\n",
        "        ***************\n",
        "\n",
        "        dense_features: (4-D Tensor)\n",
        "            The processed tensor from the respective UNet layer,\n",
        "            to be sent to the next  upsample layer.\n",
        "        \"\"\"\n",
        "        if not isFinal:\n",
        "            dense_features, dense_concat = self.DenseBlock(inputs,\n",
        "                                                           dense_layers[layer_index],\n",
        "                                                           growth_rate,\n",
        "                                                           kernel_size_d,\n",
        "                                                           dropout,\n",
        "                                                           channel_axis,\n",
        "                                                           conv_option)\n",
        "            down_outputs.append(dense_concat)\n",
        "            layer_out = self.TransitionDown(dense_concat,\n",
        "                                            dense_concat.shape[channel_axis],\n",
        "                                            dropout,\n",
        "                                            conv_option,\n",
        "                                            pool_option)\n",
        "        else:\n",
        "            conv = conv2_op(conv_option)(inputs.shape[channel_axis],\n",
        "                                         (1, 1),\n",
        "                                         padding='same')(inputs)\n",
        "            conv = BatchActivate()(conv)\n",
        "            aspp_out = atrousSPP(kernel_size_d,\n",
        "                                 inputs.shape[channel_axis],\n",
        "                                 channel_axis,\n",
        "                                 inputs.shape[channel_axis],\n",
        "                                 conv_option,\n",
        "                                 pyramid_layers)(inputs)\n",
        "            layer_out = layers.concatenate([conv, aspp_out], axis=channel_axis)\n",
        "            layer_out = conv2_op(conv_option)(inputs.shape[channel_axis],\n",
        "                                              kernel_size_d,\n",
        "                                              padding='same')(layer_out)\n",
        "            layer_out = BatchActivate()(layer_out)\n",
        "        return layer_out\n",
        "\n",
        "    def up_block_dense(self, inputs, down_outputs, kernel_size_u,\n",
        "                       kernel_size_u2, channel_axis, out_classnum, tcl_type,\n",
        "                       dense_layers, layer_index, growth_rate, dropout,\n",
        "                       conv_option, isFinal=False):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "\n",
        "        down_outputs: (List of tensors)\n",
        "            The list of tensors that contains all the downsampled inputs, which are to be\n",
        "            concatenated to the processed input tensor in the up block.\n",
        "\n",
        "        kernel_size_u: (Tuple)\n",
        "            The kernel size used for convolutions and deconvolutions.\n",
        "\n",
        "        kernel_size_u2: (Tuple)\n",
        "            The kernel size used for convolution in the final layer.\n",
        "\n",
        "        isFinal: (Boolean)\n",
        "            To check if the up_block is the final block of the Unet encoder\n",
        "            and thereby uses out_classnum in its final convolution.\n",
        "\n",
        "        ***************\n",
        "        *** Returns ***\n",
        "        ***************\n",
        "\n",
        "        dense_features: (4-D Tensor)\n",
        "            The processed tensor from the respective UNet layer,\n",
        "            to be sent to the next downsample or upsample layer.\n",
        "        \"\"\"\n",
        "        input_channels = inputs.shape[channel_axis]\n",
        "        upsample_rate = 2\n",
        "        convt = self.TransitionUp(inputs,\n",
        "                                  tcl_type,\n",
        "                                  input_channels,\n",
        "                                  conv_option,\n",
        "                                  kernel_size_u,\n",
        "                                  upsample_rate)\n",
        "        convt = layers.concatenate([convt, down_outputs[layer_index]], axis=channel_axis)\n",
        "#         convt = self.TransitionUp(inputs,\n",
        "#                                   tcl_type,\n",
        "#                                   down_outputs[layer_index].shape[channel_axis],\n",
        "#                                   conv_option,\n",
        "#                                   kernel_size_u,\n",
        "#                                   upsample_rate)\n",
        "#         convt = layers.Add()([convt, down_outputs[layer_index]])\n",
        "        layer_out = DenseLayer(dense_layers[layer_index]*growth_rate,\n",
        "                               kernel_size_u,\n",
        "                               dropout,\n",
        "                               conv_option)(convt)\n",
        "        return layer_out\n",
        "\n",
        "    def down_block_regular(self, inputs, down_outputs, kernel_size_d,\n",
        "                           channel_axis, dense_layers, layer_index, growth_rate,\n",
        "                           dropout, conv_option, pool_option, pyramid_layers,\n",
        "                           isFinal=False):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "\n",
        "        down_outputs: (List of tensors)\n",
        "            The list of tensors that contains all the downsampled inputs, which are to be\n",
        "            concatenated to the processed input tensor in the up block.\n",
        "\n",
        "        kernel_size_d: (Tuple)\n",
        "            The kernel size used for convolutions and deconvolutions.\n",
        "\n",
        "\n",
        "        ***************\n",
        "        *** Returns ***\n",
        "        ***************\n",
        "\n",
        "        dense_features: (4-D Tensor)\n",
        "            The processed tensor from the respective UNet layer,\n",
        "            to be sent to the next  upsample layer.\n",
        "        \"\"\"\n",
        "        if not isFinal:\n",
        "            conv = layers.Conv2D(self.filtersize*2**(layer_index),\n",
        "                                kernel_size_d,\n",
        "                                padding='same',\n",
        "                                kernel_initializer='he_uniform')(inputs)\n",
        "            conv = layers.Conv2D(self.filtersize*2**(layer_index),\n",
        "                                kernel_size_d,\n",
        "                                padding='same',\n",
        "                                kernel_initializer='he_uniform')(conv)\n",
        "            down_outputs.append(conv)\n",
        "            layer_out = self.TransitionDown(conv,\n",
        "                                            conv.shape[channel_axis],\n",
        "                                            dropout,\n",
        "                                            conv_option,\n",
        "                                            pool_option)\n",
        "        else:\n",
        "            conv = layers.Conv2D(self.filtersize*2**(layer_index),\n",
        "                                kernel_size_d,\n",
        "                                padding='same',\n",
        "                                kernel_initializer='he_uniform')(inputs)\n",
        "            layer_out = layers.Conv2D(self.filtersize*2**(layer_index),\n",
        "                                kernel_size_d,\n",
        "                                padding='same',\n",
        "                                kernel_initializer='he_uniform')(conv)\n",
        "        return layer_out\n",
        "\n",
        "    def up_block_regular(self, inputs, down_outputs, kernel_size_u,\n",
        "                         kernel_size_u2, channel_axis, out_classnum, tcl_type,\n",
        "                         dense_layers, layer_index, growth_rate, dropout,\n",
        "                         conv_option, isFinal=False):\n",
        "        \"\"\"\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "\n",
        "        down_outputs: (List of tensors)\n",
        "            The list of tensors that contains all the downsampled inputs, which are to be\n",
        "            concatenated to the processed input tensor in the up block.\n",
        "\n",
        "        kernel_size_u: (Tuple)\n",
        "            The kernel size used for convolutions and deconvolutions.\n",
        "\n",
        "        kernel_size_u2: (Tuple)\n",
        "            The kernel size used for convolution in the final layer.\n",
        "\n",
        "        isFinal: (Boolean)\n",
        "            To check if the up_block is the final block of the Unet encoder\n",
        "            and thereby uses out_classnum in its final convolution.\n",
        "\n",
        "        ***************\n",
        "        *** Returns ***\n",
        "        ***************\n",
        "\n",
        "        dense_features: (4-D Tensor)\n",
        "            The processed tensor from the respective UNet layer,\n",
        "            to be sent to the next downsample or upsample layer.\n",
        "        \"\"\"\n",
        "        input_channels = inputs.shape[channel_axis]\n",
        "        upsample_rate = 2\n",
        "        convt = self.TransitionUp(inputs,\n",
        "                                  tcl_type,\n",
        "                                  input_channels,\n",
        "                                  conv_option,\n",
        "                                  kernel_size_u,\n",
        "                                  upsample_rate)\n",
        "        convt = layers.concatenate([convt, down_outputs[layer_index]], axis=channel_axis)\n",
        "#         convt = self.TransitionUp(inputs,\n",
        "#                                   tcl_type,\n",
        "#                                   down_outputs[layer_index].shape[channel_axis],\n",
        "#                                   conv_option,\n",
        "#                                   kernel_size_u,\n",
        "#                                   upsample_rate)\n",
        "#         convt = layers.Add()([convt, down_outputs[layer_index]])\n",
        "        layer_out = layers.Conv2D(self.filtersize*2**(layer_index),\n",
        "                                  kernel_size_u,\n",
        "                                  padding='same',\n",
        "                                  kernel_initializer='he_uniform')(convt)\n",
        "        return layer_out\n",
        "\n",
        "    def UNet(self, select):\n",
        "        \"\"\"\n",
        "        ***************\n",
        "        *** Returns ***\n",
        "        ***************\n",
        "\n",
        "        denseunet: (tf.keras.Model object)\n",
        "            The model object for further compilation and training.\n",
        "        \"\"\"\n",
        "        inputs = tf.keras.Input(shape=self.input_shape)\n",
        "        channel_axis = self.d_format.index('C')\n",
        "        down_outputs = []\n",
        "        kernel_size_d = (3, 3)\n",
        "        kernel_size_d2 = (2, 2)\n",
        "        kernel_size_u = (3, 3)\n",
        "        kernel_size_u2 = (1, 1)\n",
        "\n",
        "        outputs = layers.Conv2D(self.filtersize,\n",
        "                                (1, 1),\n",
        "                                padding='same',\n",
        "                                kernel_initializer='he_uniform')(inputs)\n",
        "        outputs = BatchNorm(outputs)\n",
        "        for layer_index in range(self.network_depth):\n",
        "            isFinal = False if layer_index != self.network_depth-1 else True\n",
        "            if select==\"regularUNet\":\n",
        "                outputs = self.down_block_regular(outputs, down_outputs,\n",
        "                                                kernel_size_d, channel_axis,\n",
        "                                                self.dense_layers, layer_index,\n",
        "                                                self.growth_rate, self.dropout,\n",
        "                                                self.conv_option, self.pool_option,\n",
        "                                                self.pyramid_layers, isFinal=isFinal)\n",
        "            elif select==\"denseUNet\":\n",
        "                outputs = self.down_block_dense(outputs, down_outputs,\n",
        "                                                kernel_size_d, channel_axis,\n",
        "                                                self.dense_layers, layer_index,\n",
        "                                                self.growth_rate, self.dropout,\n",
        "                                                self.conv_option, self.pool_option,\n",
        "                                                self.pyramid_layers, isFinal=isFinal)\n",
        "        for layer_index in range(self.network_depth-2, -1, -1):\n",
        "            isFinal = False if layer_index !=0 else True\n",
        "            if select==\"regularUNet\":\n",
        "                outputs = self.up_block_regular(outputs, down_outputs,\n",
        "                                                kernel_size_u, kernel_size_u2,\n",
        "                                                channel_axis, self.out_classnum,\n",
        "                                                self.tcl_type, self.dense_layers,\n",
        "                                                layer_index, self.growth_rate,\n",
        "                                                self.dropout, self.conv_option,\n",
        "                                                isFinal=isFinal)\n",
        "            elif select==\"denseUNet\":\n",
        "                outputs = self.up_block_dense(outputs, down_outputs,\n",
        "                                              kernel_size_u, kernel_size_u2,\n",
        "                                              channel_axis, self.out_classnum,\n",
        "                                              self.tcl_type, self.dense_layers,\n",
        "                                              layer_index, self.growth_rate,\n",
        "                                              self.dropout, self.conv_option,\n",
        "                                              isFinal=isFinal)\n",
        "        outputs = layers.Conv2D(self.out_classnum,\n",
        "                                kernel_size_u2,\n",
        "                                padding='same',\n",
        "                                kernel_initializer='he_uniform')(outputs)\n",
        "        outputs = layers.Softmax()(outputs)\n",
        "        if select==\"regularUNet\":\n",
        "            name = 'regularUNet'\n",
        "        elif select==\"denseUNet\":\n",
        "            name = \"denseUNet\"\n",
        "        unet = tf.keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "        return unet\n",
        "\n",
        "    def Deeplab_repeat_block(self, inputs, repeats, filters,\n",
        "                             kernel_size, add_with_conv):\n",
        "        layer_input = inputs\n",
        "        final_stride = 2 if add_with_conv else 1\n",
        "        for layer in range(repeats):\n",
        "            conv = layers.SeparableConv2D(filters,\n",
        "                                          kernel_size,\n",
        "                                          padding='same')(layer_input)\n",
        "            conv = BatchActivate()(conv)\n",
        "            conv = layers.SeparableConv2D(filters,\n",
        "                                          kernel_size,\n",
        "                                          padding='same')(conv)\n",
        "            conv = BatchActivate()(conv)\n",
        "            conv = layers.SeparableConv2D(filters,\n",
        "                                          kernel_size,\n",
        "                                          strides=final_stride,\n",
        "                                          padding='same')(conv)\n",
        "            if add_with_conv:\n",
        "                conv_res = layers.Conv2D(filters,\n",
        "                                         (1, 1),\n",
        "                                         strides=final_stride,\n",
        "                                         padding='same')(layer_input)\n",
        "            else:\n",
        "                conv_res = layer_input\n",
        "            layer_input = layers.Add()([conv, conv_res])\n",
        "            layer_input = BatchActivate()(layer_input)\n",
        "        return layer_input\n",
        "\n",
        "    def Deeplab(self, pretrained):\n",
        "        \"\"\"\n",
        "        Link to the research paper that describes the structure of deeplab xception -\n",
        "        https://arxiv.org/pdf/1610.02357.pdf\n",
        "\n",
        "        Source code -\n",
        "        https://github.com/tensorflow/models/blob/master/research/deeplab/core/xception.py\n",
        "        *****************\n",
        "        *** Arguments ***\n",
        "        *****************\n",
        "        pretrained: (boolean) Selects if the entire network is to be trained or just the decoder part.\n",
        "        \"\"\"\n",
        "        inputs = tf.keras.Input(shape=self.input_shape)\n",
        "        channel_axis = self.d_format.index('C')\n",
        "        kernel_size_d = (3, 3)\n",
        "        kernel_size_d2 = (2, 2)\n",
        "        kernel_size_u = (3, 3)\n",
        "        kernel_size_u2 = (1, 1)\n",
        "\n",
        "        if not pretrained:\n",
        "            # Entry flow\n",
        "            outputs = layers.Conv2D(32, kernel_size=kernel_size_d, padding='same',\n",
        "                                    kernel_initializer='he_uniform')(inputs)\n",
        "            outputs = BatchNorm(outputs)\n",
        "            outputs = layers.Conv2D(64, kernel_size=kernel_size_d, padding='same',\n",
        "                                    kernel_initializer='he_uniform')(outputs)\n",
        "            outputs = BatchNorm(outputs)\n",
        "            outputs = self.Deeplab_repeat_block(outputs, repeats=1, filters=128,\n",
        "                                                kernel_size=kernel_size_d, add_with_conv=True)\n",
        "            outputs = self.Deeplab_repeat_block(outputs, repeats=1, filters=256,\n",
        "                                                kernel_size=kernel_size_d, add_with_conv=True)\n",
        "            downsampled_out = outputs\n",
        "            outputs = self.Deeplab_repeat_block(outputs, repeats=1, filters=728,\n",
        "                                                kernel_size=kernel_size_d, add_with_conv=True)\n",
        "            # Middle flow\n",
        "            outputs = self.Deeplab_repeat_block(outputs, repeats=16, filters=728,\n",
        "                                                kernel_size=kernel_size_d, add_with_conv=False)\n",
        "            # Exit flow\n",
        "            outputs = self.Deeplab_repeat_block(outputs, repeats=1, filters=1024,\n",
        "                                                kernel_size=kernel_size_d, add_with_conv=True)\n",
        "            outputs =layers.SeparableConv2D(1536, kernel_size=kernel_size_d, padding='same')(outputs)\n",
        "            outputs = BatchActivate()(outputs)\n",
        "            outputs = layers.SeparableConv2D(1536, kernel_size=kernel_size_d, padding='same')(outputs)\n",
        "            outputs = BatchActivate()(outputs)\n",
        "            outputs = layers.SeparableConv2D(2048, kernel_size=kernel_size_d, padding='same')(outputs)\n",
        "            outputs = BatchActivate()(outputs)\n",
        "        else:\n",
        "            base_model = keras.applications.Xception(\n",
        "                                                     include_top=False,\n",
        "                                                     weights=\"imagenet\",\n",
        "                                                     input_shape=self.input_shape,\n",
        "                                                     input_tensor=inputs\n",
        "                                                    )\n",
        "            base_model.trainable=False\n",
        "#             inputs = keras.Input(shape=self.input_shape)\n",
        "#             x = base_model(inputs, training=False)\n",
        "            outputs = base_model.get_layer('block14_sepconv2_act').output\n",
        "            downsampled_out = base_model.get_layer('block4_sepconv2_bn').output\n",
        "\n",
        "        # ASPP module\n",
        "        aspp_out = atrousSPP(kernel_size_d, 256, channel_axis, 256,\n",
        "                             self.conv_option, self.pyramid_layers)(outputs)\n",
        "#         aspp_out = layers.UpSampling2D(size=(4, 4), data_format=\"channels_last\",\n",
        "#                                        interpolation=\"bilinear\")(aspp_out)\n",
        "        aspp_out = self.TransitionUp(aspp_out, 'builtin', 256, (4, 4),\n",
        "                                     self.conv_option, upsample_rate=4)\n",
        "\n",
        "        # Decoder\n",
        "        conv = layers.Conv2D(48, (1, 1), padding='same')(downsampled_out)\n",
        "        conv = BatchNorm(conv)\n",
        "        outputs = layers.concatenate([conv, aspp_out], axis=channel_axis)\n",
        "        outputs = layers.Conv2D(256, kernel_size_u, padding='same')(outputs)\n",
        "        outputs = BatchNorm(outputs)\n",
        "        if not pretrained:\n",
        "#             outputs = layers.UpSampling2D(size=(4, 4), data_format=\"channels_last\",\n",
        "#                                           interpolation=\"bilinear\")(outputs)\n",
        "            outputs = self.TransitionUp(outputs, 'builtin', 256, (4, 4), self.conv_option,\n",
        "                                        upsample_rate=4)\n",
        "        else:\n",
        "#             outputs = layers.UpSampling2D(size=(8, 8), data_format=\"channels_last\",\n",
        "#                                           interpolation=\"bilinear\")(outputs)\n",
        "            outputs = self.TransitionUp(outputs, 'builtin', 256, (8, 8), self.conv_option,\n",
        "                                        upsample_rate=8)\n",
        "        outputs = layers.Conv2D(self.out_classnum, (1, 1), padding='same',\n",
        "                                kernel_initializer='he_uniform')(outputs)\n",
        "        outputs = layers.Softmax()(outputs)\n",
        "        if not pretrained:\n",
        "            name = 'deeplab'\n",
        "        else:\n",
        "            name = 'deeplab_pretrained'\n",
        "        deeplab = tf.keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "        return deeplab\n",
        "\n",
        "    def __call__(self, select_model):\n",
        "        if select_model==\"denseUNet\":\n",
        "            return self.UNet(select=\"denseUNet\")\n",
        "        elif select_model==\"regularUNet\":\n",
        "            return self.UNet(select=\"regularUNet\")\n",
        "        elif select_model==\"deeplab\":\n",
        "            return self.Deeplab(pretrained=False)\n",
        "        elif select_model==\"deeplab_pre\":\n",
        "            return self.Deeplab(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuD0kdBS8ed8"
      },
      "source": [
        "---\n",
        "## Display utilites\n",
        "\n",
        "* DisplayCallback class - Shows the predictions alongside the dataset and the true mask at the end of each epoch of training.\n",
        "* `create_mask()` - Creates a mask that will be used to display the predicted image, by using the fact that the label assigned to the pixel is the channel with the highest value. (since we have used the softmax function in the final layer of the model)\n",
        "* `show_predictions()` - Shows the sample image and mask when used without a model, and appends the predicted image to the list of sample image and the sample mask for the display function to display them as images.\n",
        "* `display()` function shows the image, the segmentation mask and the predicted mask (if the model is ready/is being trained) side by side, with their respective labels.\n",
        "    There are 4 modes to it:\n",
        "    1. previewing the dataset before training.\n",
        "    2. showing the training results.\n",
        "    3. showing the validation results.\n",
        "    4. showing the test results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, image_list, model, model_name, cwd):\n",
        "        super(DisplayCallback, self).__init__()\n",
        "        # self.validation_data = None\n",
        "        # self.model = None\n",
        "        # self._chief_worker_only = None\n",
        "        # self._supports_tf_logs = False\n",
        "        self.image_list = image_list\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "        self.cwd = cwd\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # clear_output(wait=True)\n",
        "        show_predictions(epoch, self.model_name, self.image_list, self.model,\n",
        "                         self.cwd, mode=1, num=3)\n",
        "        print('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                       \"image_list\": self.image_list,\n",
        "                       \"model\": self.model,\n",
        "                       \"model_name\": self.model_name,\n",
        "                       \"cwd\": self.cwd\n",
        "                       })\n",
        "\n",
        "        return config\n",
        "\n",
        "\n",
        "def create_mask(pred_mask):\n",
        "    pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
        "    pred_mask = pred_mask[..., tf.newaxis]\n",
        "    return pred_mask[0]\n",
        "\n",
        "\n",
        "def show_predictions(epoch, model_name, image_list, model, cwd,\n",
        "                     mode=0, dataset=None, num=1, WIDTH=8, HEIGHT=8):\n",
        "    \"\"\"\n",
        "    *****************\n",
        "    *** Arguments ***\n",
        "    *****************\n",
        "    epoch: (Integer)\n",
        "        Only active for 'Mode 1'. For other modes, set it to the string that is to be displayed or None.\n",
        "    model_name: (String)\n",
        "        The name of the model, as it is to be saved. (format - model_number)\n",
        "    image_list: (List)\n",
        "        The list of images, that are to be converted and displayed.\n",
        "    model: (tf.model)\n",
        "        The tensorflow model that is to be used to generate predictions.\n",
        "    mode: (Integer)\n",
        "        Sets the display type to be used.\n",
        "            * Mode '0' for images and masks only. (Preview the dataset before training)\n",
        "            * Mode '1' for images, masks and predictions. (Training dataset)\n",
        "            * Mode '2' for images, masks and predictions. (Validation dataset)\n",
        "            * Mode '3' for images and predictions only. (Test dataset)\n",
        "    dataset: (tf.dataset)\n",
        "        The dataset to preview, if any.\n",
        "    num: (Integer)\n",
        "        The number of images to be displayed.\n",
        "    \"\"\"\n",
        "    if dataset:\n",
        "        imgs_masks_and_preds = []\n",
        "        if mode == 3:\n",
        "            for image in dataset.take(num):\n",
        "                imgs_masks_and_preds.append(image[0])\n",
        "                pred_mask = model.predict(image)\n",
        "                imgs_masks_and_preds.append(create_mask(pred_mask))\n",
        "        else:\n",
        "            for image, mask in dataset.take(num):\n",
        "                imgs_masks_and_preds.append(image[0])\n",
        "                imgs_masks_and_preds.append(mask[0])\n",
        "                if mode == 1 or mode == 2:\n",
        "                    pred_mask = model.predict(image)\n",
        "                    imgs_masks_and_preds.append(create_mask(pred_mask))\n",
        "    else:\n",
        "        imgs_masks_and_preds = []\n",
        "        step = 1 if mode == 3 else 2\n",
        "        for index in range(0, len(image_list), step):\n",
        "            imgs_masks_and_preds.append(image_list[index])\n",
        "            if mode != 3:\n",
        "                imgs_masks_and_preds.append(image_list[index+1])\n",
        "            pred_mask = model.predict(image_list[index][tf.newaxis, ...])\n",
        "            imgs_masks_and_preds.append(create_mask(pred_mask))\n",
        "    display(epoch, model_name, imgs_masks_and_preds, num,\n",
        "            cwd, mode, WIDTH, HEIGHT)\n",
        "\n",
        "\n",
        "def display(epoch, model_name, display_list, num_subplts,\n",
        "            cwd, mode=0, WIDTH=8, HEIGHT=8):\n",
        "    \"\"\"\n",
        "    *****************\n",
        "    *** Arguments ***\n",
        "    *****************\n",
        "    epoch: (Integer)\n",
        "        Only active for 'Mode 1'. For other modes, set it to the string that is to be displayed or None.\n",
        "    model_name: (String)\n",
        "        The name of the model, as it is to be saved. (format - model_number)\n",
        "    display_list: (List of image data)\n",
        "        The list of images to be converted and displayed.\n",
        "    num_subplts: (Integer)\n",
        "        The number of subplots in the image.\n",
        "    mode: (Integer)\n",
        "        Sets the display type to be used.\n",
        "            * Mode '0' for images and masks only. (Preview the dataset before training)\n",
        "            * Mode '1' for images, masks and predictions. (Training dataset)\n",
        "            * Mode '2' for images, masks and predictions. (Validation dataset)\n",
        "            * Mode '3' for images and predictions only. (Test dataset)\n",
        "    \"\"\"\n",
        "\n",
        "    title_0 = ['Input Image', 'True Mask']\n",
        "    title_train = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "    title_test = ['Input Image', 'Predicted Mask']\n",
        "    num_cols = int(len(display_list)/num_subplts)\n",
        "\n",
        "    f, axes = plt.subplots(num_subplts, num_cols, facecolor='black', figsize=(WIDTH,HEIGHT))\n",
        "\n",
        "    for i in range(len(display_list)):\n",
        "        ax = plt.subplot(num_subplts, num_cols, i+1, facecolor='black')\n",
        "\n",
        "        ax.set_facecolor('black')\n",
        "        ax.xaxis.label.set_color('white')\n",
        "        ax.yaxis.label.set_color('white')\n",
        "        ax.tick_params(axis='x', colors='white')\n",
        "        ax.tick_params(axis='y', colors='white')\n",
        "        ax.spines['left'].set_color('white')\n",
        "        ax.spines['top'].set_color('white')\n",
        "        ax.spines['bottom'].set_color('white')\n",
        "        ax.spines['right'].set_color('white')\n",
        "\n",
        "        if mode == 0:\n",
        "            ax.set_title(title_0[i%num_cols], color='white')\n",
        "        elif mode == 1 or mode == 2:\n",
        "            ax.set_title(title_train[i%num_cols], color='white')\n",
        "        elif mode == 3:\n",
        "            ax.set_title(title_test[i%num_cols], color='white')\n",
        "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "\n",
        "    if type(epoch) is bool or type(epoch) is str:\n",
        "        f.suptitle(f'{epoch}', color='white')\n",
        "        if mode == 3:\n",
        "            fig_path = os.path.join(cwd, f'results', f'test_sample_results', f'{model_name}',\n",
        "                                    f'{epoch}.png')\n",
        "            try:\n",
        "                plt.savefig(fig_path)\n",
        "            except:\n",
        "                os.makedirs(os.path.dirname(fig_path))\n",
        "                plt.savefig(fig_path)\n",
        "        elif mode == 2:\n",
        "            fig_path = os.path.join(cwd, f'results', f'validation_sample_results', f'{model_name}',\n",
        "                                    f'{epoch}.png')\n",
        "            try:\n",
        "                plt.savefig(fig_path)\n",
        "            except:\n",
        "                os.makedirs(os.path.dirname(fig_path))\n",
        "                plt.savefig(fig_path)\n",
        "        elif mode == 1:\n",
        "            fig_path = os.path.join(cwd, f'results', f'training_sample_results', f'{model_name}',\n",
        "                                    f'{epoch}.png')\n",
        "            try:\n",
        "                plt.savefig(fig_path)\n",
        "            except:\n",
        "                os.makedirs(os.path.dirname(fig_path))\n",
        "                plt.savefig(fig_path)\n",
        "    else:\n",
        "        f.suptitle(f'Epoch {epoch+1}', color='white')\n",
        "        if mode == 1:\n",
        "            fig_path = os.path.join(cwd, f'results', f'training_sample_results', f'{model_name}',\n",
        "                                    f'Epoch - {epoch+1}.png')\n",
        "            try:\n",
        "                plt.savefig(fig_path)\n",
        "            except:\n",
        "                os.makedirs(os.path.dirname(fig_path))\n",
        "                plt.savefig(fig_path)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def display_test(pred_batches, PRED_LENGTH, loaded_model, model_num):\n",
        "    imgs_masks_and_preds = []\n",
        "    for image in pred_batches.take(PRED_LENGTH):\n",
        "        pred_mask = loaded_model.predict(image)\n",
        "        imgs_masks_and_preds.append((image[0], create_mask(pred_mask)))\n",
        "\n",
        "    for index, (image, pred) in enumerate(imgs_masks_and_preds):\n",
        "        display(f'Test Result {index+1}', f'model_{model_num}', [image, pred], 1,\n",
        "                cwd, mode=3, WIDTH=8, HEIGHT=8)"
      ],
      "metadata": {
        "id": "UfOLrVS_RYjx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Training utilities\n",
        "\n",
        "Sets up a learning rate scheduling function and a Garbage collecting callback class that cleans up the unnecessary variables from the memory at the end of each epoch."
      ],
      "metadata": {
        "id": "pWNbX8r2ReDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Callback To Include in Callbacks List At Training Time\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "def lr_scheduler(epoch, lr, lr_init, max_epochs):\n",
        "#     slow_iters = 0.1*max_epochs\n",
        "    power = 0.9\n",
        "#     if epoch+1 <= slow_iters:\n",
        "#         return lr_init*((epoch+1)/slow_iters)\n",
        "#     else:\n",
        "    return lr_init*(1-epoch/max_epochs)**power"
      ],
      "metadata": {
        "id": "4tXgBS6VRi2g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "---\n",
        "## Train and Test the model\n",
        "\n",
        "Now, all that is left to do is to compile and train the model.\n",
        "\n",
        "Compiling details:\n",
        "* The loss function used was `tf.keras.losses.CategoricalCrossentropy` because there are multiple classes, and the pixel values of the dataset labels are integers in [0, class_num-1]. Had the pixel been one hot encoded, we would have used `tf.keras.losses.CategoricalCrossentropy`.\n",
        "* The optimizer used is Adam, after a bit of experimentation with SGD and Adadelta.\n",
        "* The metrics used for evaluation are accuracy and mIoU. The latter carries more importance since it is directly affected in case of the checkerboard effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAeAJ-_CTUqJ"
      },
      "source": [
        "### Correction code for the MeanIOU metric\n",
        "But first, we put some code that uses subclassing to correct for the issue we had while putting `MeanIoU` as a metric during the compilation of the model.\n",
        "This code is directly copied from the github forum that discusses tensorflow problems: [tf.keras.metrics.MeanIoU have some conflicts with sparse_categorical_crossentropy](https://github.com/tensorflow/tensorflow/issues/32875).\n",
        "\n",
        "Several other posts that discuss this problem:\n",
        "* [Migrate IoU segmentaiton metrics from keras.metrics to keras_cv.metrics](https://github.com/keras-team/keras-cv/issues/909)\n",
        "* [Dimensions mismatch error when using tf.metrics.MeanIoU() with SparseCategoricalCrossEntropy loss in Tensorflow 2.2\n",
        "](https://stackoverflow.com/questions/61824470/dimensions-mismatch-error-when-using-tf-metrics-meaniou-with-sparsecategorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "X1Mre9deTSgV"
      },
      "outputs": [],
      "source": [
        "class mod_MeanIoU(tf.keras.metrics.MeanIoU):\n",
        "    \"\"\"\n",
        "    Subclassing MeanIoU from tf.keras.metrics so correct for the\n",
        "    MeanIoU measurement error that comes up during model compilation.\n",
        "\n",
        "    Link to the posts that discuss this:\n",
        "    https://github.com/tensorflow/tensorflow/issues/32875\n",
        "    https://github.com/keras-team/keras-cv/issues/909\n",
        "    https://stackoverflow.com/questions/61824470/dimensions-mismatch-error-when-using-tf-metrics-meaniou-with-sparsecategorical\n",
        "    \"\"\"\n",
        "    def __init__(self, y_true=None, y_pred=None, num_classes=None,\n",
        "                 name=None, dtype=None):\n",
        "        super().__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "        return super().update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                       \"num_classes\": self.num_classes,\n",
        "                       })\n",
        "\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5VK0E3Wa-nH"
      },
      "source": [
        "An experimental loss function that didn't work as expected, and therefore wasn't used in the final code. However it was a good learning example on how loss functions work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "C8DVO7t3tULZ"
      },
      "outputs": [],
      "source": [
        "class AsymmetricLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    A test loss function that didn't work.\n",
        "    \"\"\"\n",
        "    def __init__(self, from_logits=False, gamma_neg=4.0, gamma_pos=1.0,\n",
        "                 clip=0.10, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.from_logits = from_logits\n",
        "        self.gamma_neg = gamma_neg\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.clip = clip\n",
        "        self.eps = eps\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.dtypes.int32)\n",
        "#        Create a mask with one hot encoding for the ease of calculation.\n",
        "#         y_true = tf.convert_to_tensor(tf.keras.utils.to_categorical(y_true,\n",
        "#                                                num_classes=tf.shape(y_pred)[-1]))\n",
        "        y_true = tf.reshape(tf.one_hot(y_true, depth=tf.shape(y_pred)[-1]), tf.shape(y_pred))\n",
        "        y_pred_sig = layers.Softmax()(y_pred) if self.from_logits else y_pred\n",
        "        p = y_pred_sig\n",
        "        p_m = tf.math.maximum(p-self.clip, self.eps*tf.ones(tf.shape(p)))\n",
        "#             y_preds_neg = tf.clip_by_value(y_preds_neg+self.clip,\n",
        "#                                            clip_value_max=1-self.eps,\n",
        "#                                            clip_value_min=self.eps)\n",
        "        loss_pos = tf.math.multiply_no_nan(x=tf.math.log(tf.clip_by_value(p,\n",
        "                                                                          clip_value_min=self.eps,\n",
        "                                                                          clip_value_max=1)),\n",
        "                                           y=y_true)\n",
        "        loss_neg = tf.math.multiply_no_nan(x=tf.math.log(tf.clip_by_value(1-p_m,\n",
        "                                                                          clip_value_min=self.eps,\n",
        "                                                                          clip_value_max=1)),\n",
        "                                           y=(1-y_true))\n",
        "        loss = loss_pos + loss_neg\n",
        "        pt = tf.math.multiply_no_nan(x=(1-p)**self.gamma_pos, y=y_true) +\\\n",
        "             tf.math.multiply_no_nan(x=p_m**self.gamma_neg, y=(1-y_true))\n",
        "#         one_sided_gamma = tf.math.multiply_no_nan(x=self.gamma_pos, y=y_true) +\\\n",
        "#         tf.math.multiply_no_nan(x=self.gamma_neg, y=(1-y_true))\n",
        "#         one_sided_w = pt # **one_sided_gamma\n",
        "        loss=tf.math.multiply_no_nan(x=pt, y=loss)\n",
        "\n",
        "#         return -tf.math.reduce_sum(loss, axis=None)\n",
        "        return -tf.math.reduce_mean(tf.math.reduce_sum(loss, axis=-1))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "                       \"from_logits\": self.from_logits,\n",
        "                       \"gamma_neg\": self.gamma_neg,\n",
        "                       \"gamma_pos\": self.gamma_pos,\n",
        "                       \"clip\": self.clip,\n",
        "                       \"eps\": self.eps\n",
        "                       })\n",
        "\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSKRnutKUAn7"
      },
      "source": [
        "### Compilation and Training class\n",
        "\n",
        "* First, we define all the necessary input parameters that are to be passed to the models. These parameters are described in more detail in the code comments.\n",
        "* Using `compile_model()` - We then compile the model, where we define the loss function, optimizers and other parameters that are to be measured during the process of training. (The `from_logits` parameter in `SparseCategoricalCrossentropy`is set as false because the output of our model is from a sigmoid activation function)\n",
        "* Using `train_model()` - We train the model for a set number of epochs, steps and callback functions.\n",
        "* Using `plot_train_results()` - We plot the results of the training and validation in terms of the metrics defined in `compile_model()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6he36HK5uKAc"
      },
      "outputs": [],
      "source": [
        "class compile_and_train_model:\n",
        "    \"\"\"\n",
        "    *****************\n",
        "    *** Arguments ***\n",
        "    *****************\n",
        "    TRAIN_LENGTH: (Integer)\n",
        "        The length of the training dataset.\n",
        "\n",
        "    VAL_LENGTH: (Integer)\n",
        "        The length of the validation dataset.\n",
        "\n",
        "    EPOCHS: (Integer)\n",
        "        Number of epochs for TRAINING the model.\n",
        "\n",
        "    VAL_SUBSPLITS: (Integer)\n",
        "        Number of subsplits for the validation dataset.\n",
        "\n",
        "    output_classes: (Integer)\n",
        "        The number of output classes.\n",
        "\n",
        "    input_height: (Integer)\n",
        "        Height of the input images.\n",
        "\n",
        "    input_width: (Integer)\n",
        "        Width of the input images.\n",
        "\n",
        "    input_channels: (Integer)\n",
        "        Number of channels in the input images.\n",
        "\n",
        "    filter_init: (Integer)\n",
        "        The number of filters to be used in the first layer of the U-net.\n",
        "\n",
        "    network_depth: (Integer)\n",
        "        The depth of the Unet.\n",
        "\n",
        "    dense_unet: (Integer)\n",
        "        To choose if we want to use Dense Unet alternative,\n",
        "        if set to False we can further choose from pixel DCNs.\n",
        "\n",
        "    cl_type: (Integer)\n",
        "        0 for normal convolution, 1 for ipixel convolution.\n",
        "\n",
        "    tcl_type:(Integer)\n",
        "        0 for Regular deconv, 1 for Pixel deconv, 2 for iPixel deconv.\n",
        "\n",
        "    model_summary:(Boolean)\n",
        "        If set to true, the model flowchart and model summary are displayed.\n",
        "\n",
        "    dense_layers: (List of integers)\n",
        "        The number of dense layers in the denseblock\n",
        "        in the Unet at the depth corresponding to the list index.\n",
        "\n",
        "    growth_rate: (Integer)\n",
        "        The number of kernels/filters in each layer of the denseblock.\n",
        "\n",
        "    dropout: (Integer)\n",
        "        The dropout value for the denseblock.\n",
        "\n",
        "    conv_option: (String)\n",
        "        Sets the convolution type. Either 'conv2d' or 'adsconv2d'.\n",
        "\n",
        "    pool_option: (String)\n",
        "        Sets the pooling option. Either 'builtin' or 'conv'.\n",
        "\n",
        "    pyramid_layers: (List of integers)\n",
        "        The dilation values for atrous convolution that will be used in atrousSPP.\n",
        "\n",
        "    d_format: (String)\n",
        "        The data format of the input tensors - NHWC or NHCW.\n",
        "    \"\"\"\n",
        "    def __init__(self, TRAIN_LENGTH, VAL_LENGTH, train_batches,\n",
        "                 validation_batches, BATCH_SIZE, EPOCHS, VAL_SUBSPLITS,\n",
        "                 output_classes, input_shape, filter_init, network_depth,\n",
        "                 tcl_type, model_summary, dense_layers, growth_rate,\n",
        "                 dropout, conv_option, pool_option, pyramid_layers,\n",
        "                 model_select, cwd, model_name, d_format='NHWC'):\n",
        "\n",
        "        # input parameters to the model, based on dataset and the computation capability.\n",
        "        self.train_batches = train_batches\n",
        "        self.validation_batches = validation_batches\n",
        "        self.EPOCHS = EPOCHS\n",
        "        self.output_classes = output_classes\n",
        "        self.input_shape = input_shape\n",
        "        self.filter_init = filter_init\n",
        "        self.network_depth = network_depth\n",
        "        self.tcl_type = tcl_type\n",
        "        self.model_summary = model_summary\n",
        "        self.dense_layers = dense_layers\n",
        "        self.growth_rate = growth_rate\n",
        "        self.dropout = dropout\n",
        "        self.d_format = d_format\n",
        "        self.conv_option = conv_option\n",
        "        self.pool_option = pool_option\n",
        "        self.pyramid_layers = pyramid_layers\n",
        "        self.model_select = model_select\n",
        "        self.model_name = model_name\n",
        "        self.cwd = cwd\n",
        "\n",
        "        self.STEPS_PER_EPOCH = TRAIN_LENGTH//BATCH_SIZE\n",
        "        self.VALIDATION_STEPS =  VAL_LENGTH//BATCH_SIZE//VAL_SUBSPLITS #1\n",
        "\n",
        "    def compile_model(self, cwd):\n",
        "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "        with mirrored_strategy.scope():\n",
        "            self.miou = mod_MeanIoU(num_classes=self.output_classes, name='miou')\n",
        "            opts = [optimizers.Adam(),\n",
        "                    optimizers.SGD(momentum=0.9, decay=5e-4, nesterov=False),\n",
        "                    optimizers.Adadelta()]\n",
        "            loss_functions = [keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                              SparseCategoricalFocalLoss(gamma=2, from_logits=False),\n",
        "                              AsymmetricLoss(from_logits=False)]\n",
        "#                               sm.losses.jaccard_loss,\n",
        "#                               sm.losses.dice_loss,\n",
        "#                               sm.losses.cce_dice_loss,\n",
        "#                               sm.losses.cce_jaccard_loss,\n",
        "#                               sm.losses.categorical_focal_loss,\n",
        "#                               sm.losses.categorical_focal_dice_loss,\n",
        "#                               sm.losses.categorical_focal_jaccard_loss]\n",
        "            metrics_to_compile = ['accuracy', self.miou]\n",
        "            model = modelClass(self.input_shape, self.network_depth,\n",
        "                               self.tcl_type, self.output_classes,\n",
        "                               self.filter_init, self.dense_layers,\n",
        "                               self.growth_rate, self.dropout,\n",
        "                               self.conv_option, self.pool_option,\n",
        "                               self.pyramid_layers, self.d_format)(self.model_select)\n",
        "        model.compile(optimizer=opts[0],\n",
        "                      loss=loss_functions[0],\n",
        "                      metrics=metrics_to_compile)\n",
        "                     # run_eagerly=True)\n",
        "        if self.model_summary:\n",
        "            model.summary()\n",
        "            try:\n",
        "                image_file_path = os.path.join(cwd, f'model_details', f'model_schematics', f'{self.model_name}',\n",
        "                                               f'{self.model_name}.png')\n",
        "                tf.keras.utils.plot_model(model, to_file=image_file_path, show_shapes=False)\n",
        "            except:\n",
        "                image_file_path = os.path.join(cwd, f'model_details', f'model_schematics', f'{self.model_name}',\n",
        "                                               f'{self.model_name}.png')\n",
        "                os.makedirs(os.path.dirname(image_file_path))\n",
        "                tf.keras.utils.plot_model(model, to_file=image_file_path, show_shapes=False)\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def train_model(self, image_list, cwd, learn_r):\n",
        "        timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")\n",
        "        callback = [DisplayCallback(image_list, self.model, self.model_name, self.cwd),\n",
        "                    keras.callbacks.LearningRateScheduler(functools.partial(lr_scheduler,\n",
        "                                                                            lr_init=learn_r,\n",
        "                                                                            max_epochs=self.EPOCHS),\n",
        "                                                          verbose=1),\n",
        "                    GarbageCollectorCallback()] \\\n",
        "                   if inKaggle else [DisplayCallback(image_list, self.model, self.model_name, self.cwd),\n",
        "                                     keras.callbacks.TensorBoard(log_dir=os.path.join(cwd,\n",
        "                                                                                      f'tensorboard',\n",
        "                                                                                      f'{self.model_name}',\n",
        "                                                                                      f'{timestamp}'),\n",
        "                                     histogram_freq=1,\n",
        "                                     write_graph=True,\n",
        "                                     write_images=True,\n",
        "                                     update_freq='epoch'),\n",
        "                                     keras.callbacks.LearningRateScheduler(functools.partial(lr_scheduler,\n",
        "                                                                                             lr_init=learn_r,\n",
        "                                                                                             max_epochs=self.EPOCHS),\n",
        "                                                                           verbose=1),\n",
        "                                     GarbageCollectorCallback()]\n",
        "        self.model_history = self.model.fit(self.train_batches,\n",
        "                                            epochs=self.EPOCHS,\n",
        "                                            steps_per_epoch=self.STEPS_PER_EPOCH,\n",
        "                                            validation_steps=self.VALIDATION_STEPS,\n",
        "                                            validation_data=self.validation_batches,\n",
        "                                            callbacks=callback)\n",
        "\n",
        "    def plot_train_results(self):\n",
        "        train_loss = self.model_history.history['loss']\n",
        "        val_loss = self.model_history.history['val_loss']\n",
        "        train_iou = self.model_history.history['miou']\n",
        "        val_iou = self.model_history.history['val_miou']\n",
        "        plot_list = {'loss':(train_loss, 'Training Loss',\n",
        "                             val_loss, 'Validation Loss'),\n",
        "                     'iou':(train_iou, 'Training IOU',\n",
        "                            val_iou, 'Validation IOU')}\n",
        "\n",
        "        plt.figure(facecolor='white')\n",
        "        for plot_num, (key, (train_plot, train_label, val_plot, val_label))\\\n",
        "        in enumerate(zip(plot_list.keys(), plot_list.values())):\n",
        "            plt.subplot(len(plot_list), 1, plot_num+1)\n",
        "            plt.plot(self.model_history.epoch, train_plot,\n",
        "                     'r-*', label=train_label)\n",
        "            plt.plot(self.model_history.epoch, val_plot,\n",
        "                     'b-*', label=val_label)\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel(f\"{key}\".capitalize())\n",
        "            # plt.ylim([0, 1])\n",
        "            plt.legend()\n",
        "            plt.subplots_adjust(top=0.92, bottom=0.11, left=0.125, right=0.93, hspace=0.6, wspace=0.2)\n",
        "        try:\n",
        "            fig_path = os.path.join(self.cwd, f'results', f'training_plots', f'{self.model_name}',\n",
        "                                    f'{self.model_name} - Training Metrics vs Epochs.png')\n",
        "            plt.savefig(fig_path)\n",
        "        except:\n",
        "            fig_path = os.path.join(self.cwd, f'results', f'training_plots', f'{self.model_name}',\n",
        "                                    f'{self.model_name} - Training Metrics vs Epochs.png')\n",
        "            os.makedirs(os.path.dirname(fig_path))\n",
        "            plt.savefig(fig_path)\n",
        "        plt.show()\n",
        "\n",
        "    def __call__(self, cwd, learn_r, image_list):\n",
        "        self.compile_model(cwd)\n",
        "        show_predictions('Pre-training Predictions', self.model_name, image_list, self.model,\n",
        "                         self.cwd, mode=1, num=3)\n",
        "        self.train_model(image_list, cwd, learn_r)\n",
        "        self.plot_train_results()\n",
        "\n",
        "        model_num = '# no num in name'\n",
        "        for m in self.model_name:\n",
        "            if m.isdigit():\n",
        "                model_num = m\n",
        "        show_predictions(f'Predictions after training - Model {model_num}', self.model_name, image_list, self.model,\n",
        "                         self.cwd, dataset=self.validation_batches, mode=2, num=3)\n",
        "        # model_checkpt_path = os.path.join(cwd, f'model_details', f'saved_model',\n",
        "        #                                   f'{self.model_name}', f'saved_weights',\n",
        "        #                                   f'{self.model_name}_weights.ckpt')\n",
        "        complete_model_path = os.path.join(cwd, f'model_details', f'saved_model',\n",
        "                                           f'{self.model_name}', f'saved_complete_model')\n",
        "        self.model.evaluate(self.validation_batches)\n",
        "        try:\n",
        "            # self.model.save_weights(model_checkpt_path)\n",
        "            self.model.save(complete_model_path)\n",
        "        except:\n",
        "            # os.makedirs(os.path.dirname(model_checkpt_path))\n",
        "            # self.model.save_weights(model_checkpt_path)\n",
        "            os.makedirs(os.path.dirname(complete_model_path))\n",
        "            self.model.save(complete_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5r4RI3LeSy0"
      },
      "source": [
        "---\n",
        "## Instantiating the Entire Network\n",
        "\n",
        "We use all the major functions to see how the network works in the following cell. This approach comes from making separate python files to define, train and test your neural network in separate python files.\n",
        "\n",
        "The TensorFlow documentation gives a more direct or straightforward approach, which uses less of functions and classes and most of the code is global and a one time implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvjLHVG5Zlcb",
        "outputId": "1255e196-d1ed-4af3-8a02-9bbac9b3570c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "dataset_select = 'oxiiit'\n",
        "\n",
        "if inColab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # enter the complete path for the colab notebook.\n",
        "    cwd = '/content/drive/MyDrive/Colab Notebooks and Jamboards/Colab Notebooks/NNFL/New segmentation code'\n",
        "    if dataset_select == 'oxiiit':\n",
        "        cwd1 = f\"{cwd}/dataset_1\"\n",
        "    elif dataset_select == 'voc':\n",
        "        cwd2 = f\"{cwd}/dataset_2\"\n",
        "elif inKaggle:\n",
        "    cwd_in = '/kaggle/input'\n",
        "    cwd = '/kaggle/working'\n",
        "    if dataset_select == 'oxiiit':\n",
        "        cwd1 = f\"{cwd_in}/oxiiit\"\n",
        "    elif dataset_select == 'voc':\n",
        "        cwd2 = f\"{cwd_in}/voc2012mod\"\n",
        "else:\n",
        "    cwd = os.getcwd()\n",
        "    if dataset_select == 'oxiiit':\n",
        "        cwd1 = os.path.join(cwd, f\"dataset_1\")\n",
        "    elif dataset_select == 'voc':\n",
        "        cwd2 = os.path.join(cwd, f\"dataset_2\")\n",
        "\n",
        "# Dataset parameters\n",
        "if dataset_select == 'oxiiit':\n",
        "    BATCH_SIZE = 10\n",
        "    BUFFER_SIZE = 1000\n",
        "    load_from_tfds = True\n",
        "    img_dir = os.path.join(cwd1, f\"JPEGImages\")\n",
        "    mask_dir = os.path.join(cwd1, f\"SegmentationClass\")\n",
        "    train_txt_path = os.path.join(cwd1, f\"DatasetInfo\", f\"trainval.txt\")\n",
        "    val_txt_path = os.path.join(cwd1, f\"DatasetInfo\", f\"test.txt\")\n",
        "    input_shape = (128, 128, 3) # input_height, input_width, input_channels\n",
        "    output_classes = 3\n",
        "elif dataset_select == 'voc':\n",
        "    BATCH_SIZE = 30\n",
        "    BUFFER_SIZE = 1000\n",
        "    load_from_tfds = False\n",
        "    img_dir = os.path.join(cwd2, f\"JPEGImages\")\n",
        "    mask_dir = os.path.join(cwd2, f\"SegmentationClassFinalAug\")\n",
        "    train_txt_path = os.path.join(cwd2, f\"DatasetInfo\", f\"train.txt\")\n",
        "    val_txt_path = os.path.join(cwd2, f\"DatasetInfo\", f\"val.txt\")\n",
        "    input_shape = (128, 128, 3) # input_height, input_width, input_channels\n",
        "    output_classes = 21\n",
        "VAL_SUBSPLITS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUiNg2M_OyJW"
      },
      "source": [
        "<hr style=\"border-style: dotted;\" />\n",
        "\n",
        "### M0. Regular TCN;  M1. PixelDCN;  M2. iPixelDCN;  M3. modified iPixelDCN;  M4. Deeplab (reference)\n",
        "Instantiates all of the models mentioned in the title, with particular settings that have been selected from observing outcomes from multiple training sessions.\n",
        "\n",
        "* M0: Dense U-Net (With ASPP) with regular transposed convolution\n",
        "* M1: Dense U-Net (With ASPP) with PixelTCN\n",
        "* M2: Dense U-Net (With ASPP) with iPixelTCN\n",
        "* M3: Dense U-Net (With ASPP) with modified iPIxelTCN\n",
        "* M4: Deeplab with modified iPixelTCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ONYVrbjGa-nK"
      },
      "outputs": [],
      "source": [
        "def UNet_model(train_batches, validation_batches, input_shape, output_classes,\n",
        "               cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "               VAL_SUBSPLITS, EPOCHS):\n",
        "\n",
        "    tcl_type = 'ipixel'\n",
        "    conv_option = 'sepconv2d'\n",
        "    pool_option = 'conv'\n",
        "    model_summary = True\n",
        "    network_depth = 5\n",
        "    filter_init = 2**6\n",
        "    growth_rate = 2**4\n",
        "    dropout = 0.2\n",
        "    pyramid_layers = [1, 6, 12, 18]\n",
        "\n",
        "    dense_layers = []\n",
        "    for i in range(network_depth):\n",
        "        dense_layers.append(2**(i+2))\n",
        "\n",
        "    # Training parameters\n",
        "    models = [\"regularUNet\", \"denseUNet\", \"deeplab\", \"deeplab_pre\"]\n",
        "    model_select = models[0]\n",
        "\n",
        "    model = compile_and_train_model(TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                    train_batches, validation_batches,\n",
        "                                    BATCH_SIZE, EPOCHS,\n",
        "                                    VAL_SUBSPLITS, output_classes,\n",
        "                                    input_shape, filter_init,\n",
        "                                    network_depth, tcl_type,\n",
        "                                    model_summary, dense_layers,\n",
        "                                    growth_rate, dropout,\n",
        "                                    conv_option, pool_option,\n",
        "                                    pyramid_layers, model_select,\n",
        "                                    cwd, model_name='model_0',\n",
        "                                    d_format='NHWC')\n",
        "    return model\n",
        "\n",
        "\n",
        "def regularDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                    cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                    VAL_SUBSPLITS, EPOCHS):\n",
        "\n",
        "    tcl_type = 'builtin'\n",
        "    conv_option = 'sepconv2d'\n",
        "    pool_option = 'conv'\n",
        "    model_summary = True\n",
        "    network_depth = 5\n",
        "    filter_init = 2**6\n",
        "    growth_rate = 2**4\n",
        "    dropout = 0.2\n",
        "    pyramid_layers = [1, 6, 12, 18]\n",
        "\n",
        "    dense_layers = []\n",
        "    for i in range(network_depth):\n",
        "        dense_layers.append(2**(i+2))\n",
        "\n",
        "    # Training parameters\n",
        "    models = [\"regularUNet\", \"denseUNet\", \"deeplab\", \"deeplab_pre\"]\n",
        "    model_select = models[1]\n",
        "\n",
        "    model = compile_and_train_model(TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                    train_batches, validation_batches,\n",
        "                                    BATCH_SIZE, EPOCHS,\n",
        "                                    VAL_SUBSPLITS, output_classes,\n",
        "                                    input_shape, filter_init,\n",
        "                                    network_depth, tcl_type,\n",
        "                                    model_summary, dense_layers,\n",
        "                                    growth_rate, dropout,\n",
        "                                    conv_option, pool_option,\n",
        "                                    pyramid_layers, model_select,\n",
        "                                    cwd, model_name='model_0',\n",
        "                                    d_format='NHWC')\n",
        "    return model\n",
        "\n",
        "\n",
        "def PixelDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                  cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                  VAL_SUBSPLITS, EPOCHS):\n",
        "\n",
        "    tcl_type = 'pixel'\n",
        "    conv_option = 'sepconv2d'\n",
        "    pool_option = 'conv'\n",
        "    model_summary = True\n",
        "    network_depth = 5\n",
        "    filter_init = 2**6\n",
        "    growth_rate = 2**4\n",
        "    dropout = 0.2\n",
        "    pyramid_layers = [1, 6, 12, 18]\n",
        "\n",
        "    dense_layers = []\n",
        "    for i in range(network_depth):\n",
        "        dense_layers.append(2**(i+2))\n",
        "\n",
        "    # Training parameters\n",
        "    models = [\"regularUNet\", \"denseUNet\", \"deeplab\", \"deeplab_pre\"]\n",
        "    model_select = models[1]\n",
        "\n",
        "    model = compile_and_train_model(TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                    train_batches, validation_batches,\n",
        "                                    BATCH_SIZE, EPOCHS,\n",
        "                                    VAL_SUBSPLITS, output_classes,\n",
        "                                    input_shape, filter_init,\n",
        "                                    network_depth, tcl_type,\n",
        "                                    model_summary, dense_layers,\n",
        "                                    growth_rate, dropout,\n",
        "                                    conv_option, pool_option,\n",
        "                                    pyramid_layers, model_select,\n",
        "                                    cwd, model_name='model_1',\n",
        "                                    d_format='NHWC')\n",
        "    return model\n",
        "\n",
        "\n",
        "def iPixelDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                   cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                   VAL_SUBSPLITS, EPOCHS):\n",
        "\n",
        "    tcl_type = 'ipixel'\n",
        "    conv_option = 'sepconv2d'\n",
        "    pool_option = 'conv'\n",
        "    model_summary = True\n",
        "    network_depth = 5\n",
        "    filter_init = 2**6\n",
        "    growth_rate = 2**4\n",
        "    dropout = 0.2\n",
        "    pyramid_layers = [1, 6, 12, 18]\n",
        "\n",
        "    dense_layers = []\n",
        "    for i in range(network_depth):\n",
        "        dense_layers.append(2**(i+2))\n",
        "\n",
        "    # Training parameters\n",
        "    models = [\"regularUNet\", \"denseUNet\", \"deeplab\", \"deeplab_pre\"]\n",
        "    model_select = models[1]\n",
        "\n",
        "    model = compile_and_train_model(TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                    train_batches, validation_batches,\n",
        "                                    BATCH_SIZE, EPOCHS,\n",
        "                                    VAL_SUBSPLITS, output_classes,\n",
        "                                    input_shape, filter_init,\n",
        "                                    network_depth, tcl_type,\n",
        "                                    model_summary, dense_layers,\n",
        "                                    growth_rate, dropout,\n",
        "                                    conv_option, pool_option,\n",
        "                                    pyramid_layers, model_select,\n",
        "                                    cwd, model_name='model_2',\n",
        "                                    d_format='NHWC')\n",
        "    return model\n",
        "\n",
        "\n",
        "def modifiediPixelDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                           cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                           VAL_SUBSPLITS, EPOCHS):\n",
        "\n",
        "    tcl_type = 'modified ipixel'\n",
        "    conv_option = 'sepconv2d'\n",
        "    pool_option = 'conv'\n",
        "    model_summary = True\n",
        "    network_depth = 5\n",
        "    filter_init = 2**6\n",
        "    growth_rate = 2**4\n",
        "    dropout = 0.2\n",
        "    pyramid_layers = [1, 6, 12, 18]\n",
        "\n",
        "    dense_layers = []\n",
        "    for i in range(network_depth):\n",
        "        dense_layers.append(2**(i+2))\n",
        "\n",
        "    # Training parameters\n",
        "    models = [\"regularUNet\", \"denseUNet\", \"deeplab\", \"deeplab_pre\"]\n",
        "    model_select = models[1]\n",
        "\n",
        "    model = compile_and_train_model(TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                    train_batches, validation_batches,\n",
        "                                    BATCH_SIZE, EPOCHS,\n",
        "                                    VAL_SUBSPLITS, output_classes,\n",
        "                                    input_shape, filter_init,\n",
        "                                    network_depth, tcl_type,\n",
        "                                    model_summary, dense_layers,\n",
        "                                    growth_rate, dropout,\n",
        "                                    conv_option, pool_option,\n",
        "                                    pyramid_layers, model_select,\n",
        "                                    cwd, model_name='model_3',\n",
        "                                    d_format='NHWC')\n",
        "    return model\n",
        "\n",
        "\n",
        "def Deeplab(train_batches, validation_batches, input_shape, output_classes,\n",
        "            cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "            VAL_SUBSPLITS, EPOCHS):\n",
        "\n",
        "    tcl_type = 'modified ipixel'\n",
        "    conv_option = 'sepconv2d'\n",
        "    pool_option = 'conv'\n",
        "    model_summary = True\n",
        "    network_depth = 5\n",
        "    filter_init = 2**6\n",
        "    growth_rate = 2**4\n",
        "    dropout = 0.2\n",
        "    pyramid_layers = [1, 6, 12, 18]\n",
        "\n",
        "    dense_layers = []\n",
        "    for i in range(network_depth):\n",
        "        dense_layers.append(2**(i+2))\n",
        "\n",
        "    # Training parameters\n",
        "    models = [\"regularUNet\", \"denseUNet\", \"deeplab\", \"deeplab_pre\"]\n",
        "    model_select = models[2]\n",
        "\n",
        "    model = compile_and_train_model(TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                    train_batches, validation_batches,\n",
        "                                    BATCH_SIZE, EPOCHS,\n",
        "                                    VAL_SUBSPLITS, output_classes,\n",
        "                                    input_shape, filter_init,\n",
        "                                    network_depth, tcl_type,\n",
        "                                    model_summary, dense_layers,\n",
        "                                    growth_rate, dropout,\n",
        "                                    conv_option, pool_option,\n",
        "                                    pyramid_layers, model_select,\n",
        "                                    cwd, model_name='model_4',\n",
        "                                    d_format='NHWC')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0e-QGYGa-nM"
      },
      "source": [
        "<hr style=\"border-style: dotted;\" />\n",
        "\n",
        "### Training, Evalution and Testing\n",
        "This modules trains a model and saves it or evaluates the validation dataset with a trained model, depending on what is the value assigned to the train variable,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e7NWAtEMS28g"
      },
      "outputs": [],
      "source": [
        "def main_train(model_num, EPOCHS, learn_r):\n",
        "    (train_batches,\n",
        "    validation_batches,\n",
        "    TRAIN_LENGTH,\n",
        "    VAL_LENGTH) = load_and_split_dataset(\n",
        "        BATCH_SIZE,\n",
        "        BUFFER_SIZE,\n",
        "        load_from_tfds,\n",
        "        img_dir,\n",
        "        mask_dir,\n",
        "        train_txt_path,\n",
        "        val_txt_path,\n",
        "        input_shape[:-1]\n",
        "        )\n",
        "    samples = []\n",
        "    for images, masks in train_batches.take(3):\n",
        "        samples.append(images[0])\n",
        "        samples.append(masks[0])\n",
        "    display('Dataset Sample', 'no_models', samples, 3,\n",
        "            cwd, mode=0)\n",
        "    if model_num == 0:\n",
        "        model = regularDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                                cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                VAL_SUBSPLITS, EPOCHS)\n",
        "    elif model_num == 1:\n",
        "        model = PixelDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                                cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                VAL_SUBSPLITS, EPOCHS)\n",
        "    elif model_num == 2:\n",
        "        model = iPixelDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                                cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                VAL_SUBSPLITS, EPOCHS)\n",
        "    elif model_num == 3:\n",
        "        model = modifiediPixelDenseNet(train_batches, validation_batches, input_shape, output_classes,\n",
        "                                        cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                                        VAL_SUBSPLITS, EPOCHS)\n",
        "    else: # control model, to test.\n",
        "        model = Deeplab(train_batches, validation_batches, input_shape, output_classes,\n",
        "                        cwd, BATCH_SIZE, TRAIN_LENGTH, VAL_LENGTH,\n",
        "                        VAL_SUBSPLITS, EPOCHS)\n",
        "    model(cwd, learn_r, samples)\n",
        "\n",
        "\n",
        "def main_eval(model_num):\n",
        "    try:\n",
        "        (train_batches,\n",
        "        validation_batches,\n",
        "        TRAIN_LENGTH,\n",
        "        VAL_LENGTH) = load_and_split_dataset(\n",
        "            BATCH_SIZE,\n",
        "            BUFFER_SIZE,\n",
        "            load_from_tfds,\n",
        "            img_dir,\n",
        "            mask_dir,\n",
        "            train_txt_path,\n",
        "            val_txt_path,\n",
        "            input_shape[:-1]\n",
        "            )\n",
        "        samples = []\n",
        "        for images, masks in train_batches.take(3):\n",
        "            samples.append(images[0])\n",
        "            samples.append(masks[0])\n",
        "        display('Dataset Sample', 'no_models', samples, 3,\n",
        "                cwd, mode=0)\n",
        "\n",
        "        loaded_model = tf.keras.models.load_model(os.path.join(cwd,\n",
        "                                                                f'model_details',\n",
        "                                                                f'saved_model',\n",
        "                                                                f'model_{model_num}',\n",
        "                                                                f'saved_complete_model'),\n",
        "                                                compile=False)\n",
        "\n",
        "        miou = mod_MeanIoU(num_classes=output_classes, name='miou')\n",
        "        opts = [optimizers.Adam(),\n",
        "                optimizers.SGD(momentum=0.9, decay=5e-4, nesterov=False),\n",
        "                optimizers.Adadelta()]\n",
        "        loss_functions = [keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                        SparseCategoricalFocalLoss(gamma=2, from_logits=False),\n",
        "                        AsymmetricLoss(from_logits=False)]\n",
        "        metrics_to_compile = ['accuracy', miou]\n",
        "        loaded_model.compile(optimizer=opts[0],\n",
        "                            loss=loss_functions[0],\n",
        "                            metrics=metrics_to_compile)\n",
        "        loaded_model.evaluate(validation_batches)\n",
        "        show_predictions(f'Predictions after training - Model {model_num}', f'model_{model_num}',\n",
        "                         samples, loaded_model,\n",
        "                         cwd, dataset=validation_batches,\n",
        "                         mode=2, num=3)\n",
        "    except:\n",
        "        print(\"Model doesn't exist. Train and save the model first and then use this option.\")\n",
        "\n",
        "\n",
        "def main_test(model_num):\n",
        "    try:\n",
        "        (train_batches,\n",
        "        validation_batches,\n",
        "        pred_batches,\n",
        "        TRAIN_LENGTH,\n",
        "        VAL_LENGTH,\n",
        "        PRED_LENGTH) = load_and_split_dataset_test(\n",
        "            BATCH_SIZE,\n",
        "            BUFFER_SIZE,\n",
        "            load_from_tfds,\n",
        "            img_dir,\n",
        "            mask_dir,\n",
        "            train_txt_path,\n",
        "            val_txt_path,\n",
        "            input_shape[:-1],\n",
        "            predData_path=os.path.join(cwd, 'test_data')\n",
        "            )\n",
        "\n",
        "        samples = []\n",
        "        for images, masks in train_batches.take(3):\n",
        "            samples.append(images[0])\n",
        "            samples.append(masks[0])\n",
        "        display('Dataset Sample', 'no_models', samples, 3,\n",
        "                cwd, mode=0)\n",
        "        loaded_model = tf.keras.models.load_model(os.path.join(cwd,\n",
        "                                                                f'model_details',\n",
        "                                                                f'saved_model',\n",
        "                                                                f'model_{model_num}',\n",
        "                                                                f'saved_complete_model'),\n",
        "                                                compile=False)\n",
        "\n",
        "        miou = mod_MeanIoU(num_classes=output_classes, name='miou')\n",
        "        opts = [optimizers.Adam(),\n",
        "                optimizers.SGD(momentum=0.9, decay=5e-4, nesterov=False),\n",
        "                optimizers.Adadelta()]\n",
        "        loss_functions = [keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                            SparseCategoricalFocalLoss(gamma=2, from_logits=False),\n",
        "                            AsymmetricLoss(from_logits=False)]\n",
        "        metrics_to_compile = ['accuracy', miou]\n",
        "        loaded_model.compile(optimizer=opts[0],\n",
        "                                loss=loss_functions[0],\n",
        "                                metrics=metrics_to_compile)\n",
        "        display_test(pred_batches, PRED_LENGTH, loaded_model, model_num)\n",
        "    except:\n",
        "        print(\"Model doesn't exist. Train and save the model first and then use this option.\")\n",
        "\n",
        "model_num = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdbqT4w4S28i"
      },
      "source": [
        "<hr style=\"border-style: dotted;\" />\n",
        "\n",
        "#### Training\n",
        "Training the selected model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K41xGQNhS28i"
      },
      "outputs": [],
      "source": [
        "learn_r = 1e-3\n",
        "EPOCHS = 10\n",
        "main_train(model_num, EPOCHS, learn_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu90WSXCS28j"
      },
      "source": [
        "<hr style=\"border-style: dotted;\" />\n",
        "\n",
        "#### Evaluation\n",
        "Evaluating the trained/pre-trained model on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy0zEgpgS28q"
      },
      "outputs": [],
      "source": [
        "main_eval(model_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vhnxLPaS28r"
      },
      "source": [
        "<hr style=\"border-style: dotted;\" />\n",
        "\n",
        "#### Testing\n",
        "Testing the trained/pre-trained model on test data (a smaller set of images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG0dIe5tS28r"
      },
      "outputs": [],
      "source": [
        "main_test(model_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LBVkIhpa-nX"
      },
      "source": [
        "Compressing all of the output as a zip file, so that it can be downloaded easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoYt1ryKa-nX"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "OUTPUT_NAME = 'outputs'\n",
        "DIRECTORY_TO_ZIP = '/kaggle/working/results'\n",
        "shutil.make_archive(OUTPUT_NAME, 'zip', DIRECTORY_TO_ZIP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2LRjcVYeHUa"
      },
      "source": [
        "<hr style=\"border-style: dotted;\" />\n",
        "### Uploading to Tensorboard\n",
        "Uploading the data to Tensorboard. (This section is not included in the python code because VS code has an extension that helps visualize tensorboard logs, and this Tensorboard doesn't work with Kaggle, therefore this part is only relevant for Colab use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlF8wzgnWaY1"
      },
      "outputs": [],
      "source": [
        "!tensorboard dev upload --logdir ./logs \\\n",
        "  --name \"Segmentation on Oxford-IIIT Pets dataset\" \\\n",
        "  --description \"Training results from https://colab.research.google.com/drive/1tNoyaNEeSrbHPaUKVxwTEooHeCnVVL5P#scrollTo=BlF8wzgnWaY1\" \\\n",
        "  --one_shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf_AuLeNNx1d"
      },
      "source": [
        "---\n",
        "## Clear memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkuYjGlDN2_N"
      },
      "outputs": [],
      "source": [
        "!pip install numba\n",
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAuGnegya-nZ"
      },
      "source": [
        "---\n",
        "---\n",
        "# Extras\n",
        "Code used for modifying the PASCAL VOC dataset so that it can be pre processed appropriately for it to be put through the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AM_vkSr-ngz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from numba import jit, prange\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "@jit(nopython=True, nogil=True, parallel=True)\n",
        "def img_mod_parallel(image):\n",
        "    #if len(image) is not None:\n",
        "    for j_num in prange(len(image)):\n",
        "        #if len(image[j_num]) is not None:\n",
        "        for k_num in prange(len(image[j_num])):\n",
        "            if list(image[j_num][k_num]) == [0,0,0]:\n",
        "                image[j_num][k_num] = 1\n",
        "    return image\n",
        "\n",
        "@jit(nopython=True, nogil=True)\n",
        "def img_mod(image):\n",
        "    for j_num, j in enumerate(image):\n",
        "        for k_num, k in enumerate(j):\n",
        "            if list(k) == [0,0,0]:\n",
        "                    image[j_num, k_num] = 1\n",
        "    return image\n",
        "\n",
        "main_dir =  '/content/drive/MyDrive/Colab Notebooks and Jamboards/Colab Notebooks/NNFL/New segmentation code'\n",
        "filenames = os.listdir(f\"{main_dir}/dataset_2/SegmentationClassFinal3\")\n",
        "t0 = time.time()\n",
        "images = [cv2.imread(f\"{main_dir}/dataset_2/SegmentationClassFinal3/{elem}\") for elem in filenames]\n",
        "\n",
        "for index, (filename ,image) in enumerate(zip(filenames, images)):\n",
        "    print(index, filename)\n",
        "    # for j_num, j in enumerate(image):\n",
        "    #     for k_num, k in enumerate(j):\n",
        "    #         if list(k) == [0,0,0]:\n",
        "    #                 image[j_num, k_num] = 1\n",
        "    final_image = img_mod_parallel(image)\n",
        "    imageio.imwrite(f\"{main_dir}/dataset_2/SegmentationClassFinal3/{filename}\", final_image.astype(np.uint8))\n",
        "\n",
        "t1 = time.time()\n",
        "print(f\"total_time:{t1-t0}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}